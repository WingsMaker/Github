1,"A machine learning team has several large CSV datasets in Amazon S3. Historically, models built with the Amazon SageMaker Linear Learner algorithm have taken hours to train on similar-sized datasets. The team's leaders need to accelerate the training process.
What can a machine learning specialist do to address this concern?

A) Use Amazon SageMaker Pipe mode.

B) Use Amazon Machine Learning to train the models.

C) Use Amazon Kinesis to stream the data to Amazon SageMaker.

D) Use AWS Glue to transform the CSV dataset to the JSON format.","Answer : A

A  Amazon SageMaker Pipe mode streams the data directly to the container, 
which improves the performance of training jobs. 
In Pipe mode, your training job streams data directly from Amazon S3. 
Streaming can provide faster start times for training jobs and better throughput.
With Pipe mode, you also reduce the size of the Amazon EBS volumes for your training instances. 

B would not apply in this scenario. 
C is a streaming ingestion solution, but is not applicable in this scenario. 
D transforms the data structure.
",
2,"A term frequency–inverse document frequency (tf–idf) matrix using both unigrams and bigrams is built from a text corpus consisting of the following two sentences:
1. Please call the number below.
2. Please do not call us.
What are the dimensions of the tf–idf matrix?

A) (2, 16)

B) (2, 8)

C) (2, 10)

D) (8, 10)
","Answer : A

A – There are 2 sentences, 8 unique unigrams, and 8 unique bigrams, 
so the result would be (2,16). 
The phrases are ""Please call the number below""
and ""Please do not call us""

Each word individually (unigram) is 
Please,call,the,number,below,do,not,us

The unique bigrams are 
Please call,call the,the number,number below,
Please do,do not,not call,call us.",
3,"A company is setting up a system to manage all of the datasets it stores in Amazon S3. The company would like to automate running transformation jobs on the data and maintaining a catalog of the metadata concerning the datasets. The solution should require the least amount of setup and maintenance.
Which solution will allow the company to achieve its goals?

A) Create an Amazon EMR cluster with Apache Hive installed. Then, create a Hive metastore and a script to run transformation jobs on a schedule.

B) Create an AWS Glue crawler to populate the AWS Glue Data Catalog. Then, author an AWS Glue ETL job, and set up a schedule for data transformation jobs.

C) Create an Amazon EMR cluster with Apache Spark installed. Then, create an Apache Hive metastore and a script to run transformation jobs on a schedule.


D) Create an Amazon SageMaker Jupyter notebook instance that transforms the data. Then, create an Apache Hive metastore and a script to run transformation jobs on a schedule.","Answer : B
  
B – AWS Glue requires the least amount of setup and maintenance 
since it is serverless, and it does not require management of 
the infrastructure. Refer to this link for supporting information. 
A, C, and D are all solutions that can solve the problem, 
but require more steps for configuration, and
require higher operational overhead to run and maintain.
",
4,"A data scientist is evaluating different binary classification models. A false positive result is 5 times more expensive (from a business perspective) than a false negative result.
The models should be evaluated based on the following criteria:
1) Must have a recall rate of at least 80%
2) Must have a false positive rate of 10% or less
3) Must minimize business costs
After creating each binary classification model, the data scientist generates the corresponding confusion
matrix.
Which confusion matrix represents the model that satisfies the requirements?

A) TN = 91, FP = 9, FN = 22, TP = 78

B) TN = 99, FP = 1, FN = 21, TP = 79

C) TN = 96, FP = 4, FN = 10, TP = 90

D) TN = 98, FP = 2, FN = 18, TP = 82","Answers : D
  
TP = True Positive				
FP = False Positive				
FN = False Negative				
TN = True Negative				
RR = Recall Rate = TP / (TP + FN)				
FPR = False Positive Rate (FPR) = FP / (FP + TN)				
Cost = 5 * FP + FN				
(1) Must have a recall rate of at least 80%				
(2) Must have a false positive rate of 10% or less				
(3) Must minimize business costs
Minimum Cost = MINIFS(I14:I18,J14:J18,""Yes"",K14:K18,""Yes"")				
               A          B          C          D
TN         91          99          96          98
FP          9            1            4             2
FN         22          21          10          18
TP         78           79          90          82
RR         0.78      0.79        0.9       0.82
FPR      0.09       0.01        0.04     0.02
Cost       67          26          30          28
RR >= 0.8 ? => C,D
FPR <= 0.1 ? => A,B,C,D
Minimum Cost ? => D
Options C and D have a recall greater than 80% and an FPR less than 10%, but D is the most cost effective.
",
5,"A data scientist uses logistic regression to build a fraud detection model. While the model accuracy is 99%, 90% of the fraud cases are not detected by the model.
What action will definitively help the model detect more than 10% of fraud cases?

A) Using undersampling to balance the dataset

B) Decreasing the class probability threshold

C) Using regularization to reduce overfitting

D) Using oversampling to balance the dataset
","Answers : B
  
B – Decreasing the class probability threshold makes the model more sensitive and, therefore, marks more cases as the positive class, which is fraud in this case. This will increase the likelihood of fraud detection.
However, it comes at the price of lowering precision. 
Example :
# changing threshold and predicting 
print('prediction with higher threshold 0.9 :') 
y_pred_new_threshold = (lr.predict_proba(X_test)[:, 1] >= 0.9).astype(int) 
print(y_pred_new_threshold)",
6,"A Machine Learning Specialist is implementing a full Bayesian network on a dataset that describes public transit in New York City. One of the random variables is discrete, and represents the number of minutes New Yorkers wait for a bus given that the buses cycle every 10 minutes, with a mean of 3 minutes.
Which prior probability distribution should the ML Specialist use for this variable?

A. Poisson distribution 

B. Uniform distribution

C. Normal distribution

D. Binomial distribution
","Answers : D

The main difference between the binomial distribution and the normal distribution is that binomial distribution is discrete, whereas the normal distribution is continuous.
",
7,"An interactive online dictionary wants to add a widget that displays words used in similar contexts. A Machine Learning Specialist is asked to provide word features for the downstream nearest neighbor model powering the widget.
What should the Specialist do to meet these requirements?

A. Create one-hot word encoding vectors.

B. Produce a set of synonyms for every word using Amazon Mechanical Turk.

C. Create word embedding factors that store edit distance with every other word.

D. Download word embedding's pre-trained on a large corpus.
","Answers : A
",
8,"A Machine Learning team uses Amazon SageMaker to train an Apache MXNet handwritten digit classifier model using a research dataset. The team wants to receive a notification when the model is overfitting. Auditors want to view the Amazon SageMaker log activity report to ensure there are no unauthorized API calls. What should the Machine Learning team do to address the requirements with the least amount of code and fewest steps?

A. Implement an AWS Lambda function to log Amazon SageMaker API calls to Amazon S3. Add code to push a custom metric to Amazon CloudWatch

B. Create an alarm in CloudWatch with Amazon SNS to receive a notification when the model is overfitting.

C. Use AWS CloudTrail to log Amazon SageMaker API calls to Amazon S3. Add code to push a custom metric to Amazon CloudWatch

D. Create an alarm in CloudWatch with Amazon SNS to receive a notification when the model is overfitting.

E. Implement an AWS Lambda function to log Amazon SageMaker API calls to AWS CloudTrail

F. Add code to push a custom metric to Amazon CloudWatch

G. Create an alarm in CloudWatch with Amazon SNS to receive a notification when the model is overfitting.

H. Use AWS CloudTrail to log Amazon SageMaker API calls to Amazon S3. Set up Amazon SNS to receive a notification when the model is overfitting.
","Answers : C
",
9,"A gaming company has launched an online game where people can start playing for free but they need to pay if they choose to use certain features The company needs to build an automated system to predict whether or not a new user will become a paid user within 1 year The company has gathered a labeled dataset from 1 million users.
The training dataset consists of 1000 positive samples (from users who ended up paying within 1 year) and 999 negative samples (from users who did not use any paid features) Each data sample consists of 200 features including user age, device, location, and play patterns.
Using this dataset for training, the Data Science team trained a random forest model that converged with over 99% accuracy on the training set However, the prediction results on a test dataset were not satisfactory.
Which of the following approaches should the Data Science team take to mitigate this issue? (Select TWO.)

A. Add more deep trees to the random forest to enable the model to learn more features.

B. indicate a copy of the samples in the test database in the training dataset

C. Generate more positive samples by duplicating the positive samples and adding a small amount of noise to the duplicated data.

D. Change the cost function so that false negatives have a higher impact on the cost value than false positives

E. Change the cost function so that false positives have a higher impact on the cost value than false negatives","Answers : B,D
",
10,"A Machine Learning Specialist is developing recommendation engine for a photography blog Given a picture, the recommendation engine should show a picture that captures similar objects The Specialist would like to create a numerical representation feature to perform nearest-neighbor searches.
What actions would allow the Specialist to get relevant numerical representations?

A. Reduce image resolution and use reduced resolution pixel values as features

B. Use Amazon Mechanical Turk to label image content and create a one-hot representation indicating the presence of specific labels

C. Run images through a neural network pie-trained on ImageNet, and collect the feature vectors from the penultimate layer

D. Average colors by channel to obtain three-dimensional representations of images.
","Answers : A
",
11,"A company wants to classify user behavior as either fraudulent or normal. Based on internal research, a Machine Learning Specialist would like to build a binary classifier based on two features: age of account and transaction month. 
The class distribution for these features is illustrated in the figure provided.

Based on this information which model would have the HIGHEST accuracy?

A. Long short-term memory (LSTM) model with scaled exponential linear unit (SELL))

B. Logistic regression

C. Support vector machine (SVM) with non-linear kernel

D. Single perceptron with tanh activation function","Answers : B
",qn11.jpg
12,"The displayed graph is from a foresting model for testing a time series.
Considering the graph only, which conclusion should a Machine Learning Specialist make about the behavior of the model?

A. The model predicts both the trend and the seasonality well.

B. The model predicts the trend well, but not the seasonality.

C. The model predicts the seasonality well, but not the trend.

D. The model does not predict the trend or the seasonality well.","Answers : D
",qn12.jpg
13,"A Machine Learning Specialist observes several performance problems with the training portion of a machine learning solution on Amazon SageMaker The solution uses a large training dataset 2 TB in size and is using the SageMaker k-means algorithm The observed issues include the unacceptable length of time it takes before the training job launches and poor I/O throughput while training the model.
What should the Specialist do to address the performance issues with the current solution?

A. Use the SageMaker batch transform feature

B. Compress the training data into Apache Parquet format.

C. Ensure that the input mode for the training job is set to Pipe.

D. Copy the training dataset to an Amazon EFS volume mounted on the SageMaker instance.
","Answers : D
",
14,"A Machine Learning Specialist is working for a credit card processing company and receives an unbalanced dataset containing credit card transactions. It contains 99,000 valid transactions and 1,000 fraudulent transactions The Specialist is asked to score a model that was run against the dataset The Specialist has been advised that identifying valid transactions is equally as important as identifying fraudulent transactions What metric is BEST suited to score the model?

A. Precision

B. Recall

C. Area Under the ROC Curve (AUC)

D. Root Mean Square Error (RMSE)
","Answers : A

Precision = TruePositives / (TruePositives + FalsePositives)
",
15,"A Machine Learning Specialist is creating a new natural language processing application that processes a dataset comprised of 1 million sentences The aim is to then run Word2Vec to generate embeddings of the sentences and enable different types of predictions
Here is an example from the dataset
""The quck BROWN FOX jumps over the lazy dog ""
Which of the following are the operations the Specialist needs to perform to correctly sanitize and prepare the data in a repeatable manner? (Select THREE)

A. Perform part-of-speech tagging and keep the action verb and the nouns only

B. Normalize all words by making the sentence lowercase

C. Remove stop words using an English stopword dictionary.

D. Correct the typography on ""quck"" to ""quick.""

E. One-hot encode all words in the sentence

F. Tokenize the sentence into words.
","Answers : A,B,D
",
16,"A company is running a machine learning prediction service that generates 100 TB of predictions every day A Machine Learning Specialist must generate a visualization of the daily precision-recall curve from the predictions, and forward a read-only version to the Business team.
Which solution requires the LEAST coding effort?

A. Run a daily Amazon EMR workflow to generate precision-recall data, and save the results in Amazon S3 Give the Business team read-only access to S3

B. Generate daily precision-recall data in Amazon QuickSight, and publish the results in a dashboard shared with the Business team

C. Run a daily Amazon EMR workflow to generate precision-recall data, and save the results in Amazon S3 Visualize the arrays in Amazon QuickSight, and publish them in a dashboard shared with the Business team

D. Generate daily precision-recall data in Amazon ES (ElasticSearch), and publish the results in a dashboard shared with the Business team.
","Answers : C
",
17,"A company has raw user and transaction data stored in Amazon S3 a MySQL database, and Amazon RedShift A Data Scientist needs to perform an analysis by joining the three datasets from Amazon S3, MySQL, and Amazon RedShift, and then calculating the average-of a few selected columns from the joined data
Which AWS service should the Data Scientist use?

A. Amazon Athena

B. Amazon Redshift Spectrum

C. AWS Glue

D. Amazon QuickSight
","Answers : A

Athena helps you analyze unstructured, semi-structured, and structured data stored in Amazon S3. Examples include CSV, JSON, or columnar data formats such as Apache Parquet and Apache ORC. You can use Athena to run ad-hoc queries using ANSI SQL, without the need to aggregate or load the data into Athena.
",
18,"A Data Scientist is developing a machine learning model to classify whether a financial transaction is fraudulent. The labeled data available for training consists of 100,000 non-fraudulent observations and 1,000 fraudulent observations.
The Data Scientist applies the XGBoost algorithm to the data, resulting in the following confusion matrix when the trained model is applied to a previously unseen validation dataset. The accuracy of the model is 99.1%, but the Data Scientist has been asked to reduce the number of false negatives.

Predicted                0       1
Actual         0  99,966    34
                      1        877  123

Which combination of steps should the Data Scientist take to reduce the number of false positive predictions by the model? (Select TWO.)

A. Change the XGBoost eval_metric parameter to optimize based on rmse instead of error.

B. Increase the XGBoost scale_pos_weight parameter to adjust the balance of positive and negative weights.

C. Increase the XGBoost max_depth parameter because the model is currently underfitting the data.

D. Change the XGBoost eval_metric parameter to optimize based on AUC instead of error.

E. Decrease the XGBoost max_depth parameter because the model is currently overfitting the data.
","Answers : D,E

Example :
params = {
 'objective': 'multi:softprob',
 'tree_method': 'gpu_hist',
 'num_class': 27,
 'seed': 0,
 'max_depth': 2,
 'colsample_bytree': 0.36524046160303747,
 'colsample_bylevel': 0.7008644188368828,
 'grow_policy': 'lossguide',
 'lambda': 1-08,
 'alpha': 0.1,
 'subsample': 0.9,
 'eta': 0.01,
 'eval_metric': 'merror'}

xgb.train(params, dX_train, 
          num_boost_round=100, 
          verbose_eval=10, 
          early_stopping_rounds=10, 
          evals=[(dX_train, 'train') , (dX_valid, 'valid')],
          )
",
19,"When submitting Amazon SageMaker training jobs using one of the built-in algorithms, which common parameters MUST be specified? (Select THREE.)

A. The training channel identifying the location of training data on an Amazon S3 bucket.

B. The validation channel identifying the location of validation data on an Amazon S3 bucket.

C. The IAM role that Amazon SageMaker can assume to perform tasks on behalf of the users.

D. Hyperparameters in a JSON array as documented for the algorithm used.

E. The Amazon EC2 instance class specifying whether training will be run using CPU or GPU.

F. The output path specifying where on an Amazon S3 bucket the trained model will persist.
","Answers : A,E,F
",
20,"A Machine Learning Specialist is working with multiple data sources containing billions of records that need to be joined. What feature engineering and model development approach should the Specialist take with a dataset this large?

A. Use an Amazon SageMaker notebook for both feature engineering and model development

B. Use an Amazon SageMaker notebook for feature engineering and Amazon ML for model development

C. Use Amazon EMR for feature engineering and Amazon SageMaker SDK for model development

D. Use Amazon ML for both feature engineering and model development.
","Answers : B
",
21,"An employee found a video clip with audio on a company's social media feed. The language used in the video is Spanish. English is the employee's first language, and they do not understand Spanish. The employee wants to do a sentiment analysis.
What combination of services is the MOST efficient to accomplish the task?

A. Amazon Transcribe, Amazon Translate, and Amazon Comprehend

B. Amazon Transcribe, Amazon Comprehend, and Amazon SageMaker seq2seq

C. Amazon Transcribe, Amazon Translate, and Amazon SageMaker Neural Topic Model (NTM)

D. Amazon Transcribe, Amazon Translate, and Amazon SageMaker BlazingText
","Answers : C
",
22,"An e-commerce company needs a customized training model to classify images of its shirts and pants products The company needs a proof of concept in 2 to 3 days with good accuracy Which compute choice should the Machine Learning Specialist select to train and achieve good accuracy on the model quickly?

A. . m5 4xlarge (general purpose)

B. r5.2xlarge (memory optimized)

C. p3.2xlarge (GPU accelerated computing)

D. p3 8xlarge (GPU accelerated computing)
","Answers : C
",
23,"Given the following confusion matrix for a movie classification model, what is the true class frequency for Romance and the predicted class frequency for Adventure?

True                Predicted Values
Values:      Romance   Thriller  Adventure   Total    (k)        F1 Score
Romance          44.92        5.41        7.59         57.92    49.11     0.78
Thriller              16.46        1.98        2.79         21.23    18.00     0.33
Adventure        16.18        1.94        2.73         20.85    17.68     0.32
Total                   77.56        9.33        13.11        100%     84.8	
(k)                        65.77        7.91        11.12        84.8		


A. The true class frequency for Romance is 77.56% and the predicted class frequency for Adventure is 20.85%

B. The true class frequency for Romance is 57.92% and the predicted class frequency for Adventure is 13.12%

C. The true class frequency for Romance is 0 78 and the predicted class frequency for Adventure is (0 47 - 0.32).

D. The true class frequency for Romance is 77.56% * 0.78 and the predicted class frequency for Adventure is 20.85% * 0.32
","Answers : B

https://docs.aws.amazon.com/machine-learning/latest/dg/multiclass-model-insights.html

The true class frequency for Romance is at horizontal total sum :
57.92% 
and the predicted class frequency for Adventure is at vertical total sum :
13.12%

Looks like this F1 score in the example is not correct.",
24,"A Machine Learning Specialist needs to be able to ingest streaming data and store it in Apache Parquet files for exploration and analysis. Which of the following services would both ingest and store this data in the correct format?

A. AWS DMS (Database Migration Service )

B. Amazon Kinesis Data Streams

C. Amazon Kinesis Data Firehose

D. Amazon Kinesis Data Analytics
","Answers : C
",
25,"This graph shows the training and validation loss against the epochs for a neural network The network being trained is as follows
• Two dense layers one output neuron
• 100 neurons in each layer
• 100 epochs
• Random initialization of weights
Which technique can be used to improve model performance in terms of accuracy in the validation set?

A. Early stopping

B. Random initialization of weights with appropriate seed

C. Increasing the number of epochs

D. Adding another layer with the 100 neurons
","Answers : D
",qn25.jpg
26,"A bank's Machine Learning team is developing an approach for credit card fraud detection The company has a large dataset of historical data labeled as fraudulent 
The goal is to build a model to take the information from new transactions and predict whether each transaction is fraudulent or not
Which built-in Amazon SageMaker machine learning algorithm should be used for modeling this problem?

A. Seq2seq

B. XGBoost

C. K-means

D. Random Cut Forest (RCF)
","Answers : C
",
27,"IT leadership wants Jo transition a company's existing machine learning data storage environment to AWS as a temporary ad hoc solution The company currently uses a custom software process that heavily leverages SOL as a query language and exclusively stores generated csv documents for machine learning
The ideal state for the company would be a solution that allows it to continue to use the current workforce of SQL experts The solution must also support the storage of csv and JSON files, and be able to query over semi-structured data The following are high priorities for the company:
• Solution simplicity
• Fast development time
• Low cost
• High flexibility
What technologies meet the company? requirements?

A. Amazon S3 and Amazon Athena

B. Amazon Redshift and AWS Glue

C. Amazon DynamoDB and DynamoDB Accelerator (DAX)

D. Amazon RDS and Amazon ES
","Answers : B
",
28,"A manufacturer of car engines collects data from cars as they are being driven The data collected includes timestamp, engine temperature, rotations per minute (RPM), and other sensor readings The company wants to predict when an engine is going to have a problem so it can notify drivers in advance to get engine maintenance The engine data is loaded into a data lake for training
Which is the MOST suitable predictive model that can be deployed into production?

A. Add labels over time to indicate which engine faults occur at what time in the future to turn this into a supervised learning problem Use a recurrent neural network (RNN) to train the model to recognize when an engine might need maintenance for a certain fault.

B. This data requires an unsupervised learning algorithm Use Amazon SageMaker k-means to cluster the data

C. Add labels over time to indicate which engine faults occur at what time in the future to turn this into a supervised learning problem Use a convolutional neural network (CNN) to train the model to recognize when an engine might need maintenance for a certain fault.

D. This data is already formulated as a time series Use Amazon SageMaker seq2seq to model the time series.
","Answers : B
",
29,"A Machine Learning Specialist has built a model using Amazon SageMaker built-in algorithms and is not getting expected accurate results The Specialist wants to use hyperparameter optimization to increase the model's accuracy
Which method is the MOST repeatable and requires the LEAST amount of effort to achieve this?

A. Launch multiple training jobs in parallel with different hyperparameters

B. Create an AWS Step Functions workflow that monitors the accuracy in Amazon CloudWatch Logs and relaunches the training job with a defined list of hyperparameters

C. Create a hyperparameter tuning job and set the accuracy as an objective metric.

D. Create a random walk in the parameter space to iterate through a range of values that should be used for each individual hyperparameter
","Answers : B
",
30,"A Data Scientist is developing a machine learning model to predict future patient outcomes based on information collected about each patient and their treatment plans. The model should output a continuous value as its prediction. The data available includes labeled outcomes for a set of 4,000 patients. The study was conducted on a group of individuals over the age of 65 who have a particular disease that is known to worsen with age.
Initial models have performed poorly. While reviewing the underlying data, the Data Scientist notices that, out of 4,000 patient observations, there are 450 where the patient age has been input as 0. The other features for these observations appear normal compared to the rest of the sample population.
How should the Data Scientist correct this issue?

A. Drop all records from the dataset where age has been set to 0.

B. Replace the age field value for records with a value of 0 with the mean or median value from the dataset.

C. Drop the age feature from the dataset and train the model using the rest of the features.

D. Use k-means clustering to handle missing features.
","Answers : A
",
31,"A company is using Amazon Polly to translate plaintext documents to speech for automated company announcements However company acronyms are being mispronounced in the current documents How should a Machine Learning Specialist address this issue for future documents?

A. Convert current documents to SSML(Speech Synthesis Markup Language) with pronunciation tags.

B. Create an appropriate pronunciation lexicon.

C. Output speech marks to guide in pronunciation

D. Use Amazon Lex to preprocess the text files for pronunciation
","Answers : A
",
32,"An Machine Learning Specialist discover the following statistics while experimenting on a model.
What can the Specialist conclude from the experiments?

Experiment 1 :
Baselin Model
Train Error = 5%
Test Error = 16%

Experiment 2 :
The Specialist added more layers and neurons to the model
and received the following results
Train Error = 5.2%
Test Error = 15.7%

Experiment 3 :
The Specialist reverted back to the original number of neurons
from experiment 1 and implemented regularization in the neural network,
which yield the following results
Train Error = 4.7%
Test Error = 9.5%

A. The model In Experiment 1 had a high variance error lhat was reduced in Experiment 3 by regularization Experiment 2 shows that there is minimal bias error in Experiment 1

B. The model in Experiment 1 had a high bias error that was reduced in Experiment 3 by regularization Experiment 2 shows that there is minimal variance error in Experiment 1

C. The model in Experiment 1 had a high bias error and a high variance error that were reduced in Experiment 3 by regularization Experiment 2 shows thai high bias cannot be reduced by increasing layers and neurons in the model

D. The model in Experiment 1 had a high random noise error that was reduced in Expenment 3 by regularization Expenment 2 shows that random noise cannot be
reduced by increasing layers and neurons in the model
","Answers : C
",
33,"A Machine Learning Specialist is preparing data for training on Amazon SageMaker The Specialist is transformed into a numpy .array, which appears to be negatively affecting the speed of the training
What should the Specialist do to optimize the data for training on SageMaker?

A. Use the SageMaker batch transform feature to transform the training data into a DataFrame

B. Use AWS Glue to compress the data into the Apache Parquet format

C. Transform the dataset into the Recordio protobuf format

D. Use the SageMaker hyperparameter optimization feature to automatically optimize the data
","Correct Answer
C. Transform the dataset into the Recordio protobuf format
Explanation
The Specialist should transform the dataset into the Recordio protobuf format. This format is optimized for high performance, efficient data storage and retrieval, which can improve the speed of training on SageMaker.",
34,"A Machine Learning Specialist has created a deep learning neural network model that performs well on the training data but performs poorly on the test data.
Which of the following methods should the Specialist consider using to correct this? (Select THREE.)

A. Decrease regularization.

B. Increase regularization.

C. Increase dropout.

D. Decrease dropout.

E. Increase feature combinations.

F. Decrease feature combinations.
","Answers : B,D,E
",
35,"A large mobile network operating company is building a machine learning model to predict customers who are likely to unsubscribe from the service. The company plans to offer an incentive for these customers as the cost of churn is far greater than the cost of the incentive.
The model produces the following confusion matrix after evaluating on a test dataset of 100 customers: Based on the model evaluation results, why is this a viable model for production?

n = 100                       ACTUAL Churn    PREDICTED CHURN
                                      Yes                         No
Actual Churn Yes     10                            4
Actual No                   10                         76

A. The model is 86% accurate and the cost incurred by the company as a result of false negatives is less than the false positives.

B. The precision of the model is 86%, which is less than the accuracy of the model.

C. The model is 86% accurate and the cost incurred by the company as a result of false positives is less than the false negatives.

D. The precision of the model is 86%, which is greater than the accuracy of the model.
","Answers : B
",
36,"Which of the following metrics should a Machine Learning Specialist generally use to compare/evaluate machine learning classification models against each other?

A. Recall

B. Misclassification rate

C. Mean absolute percentage error (MAPE)

D. Area Under the ROC Curve (AUC)
","Answers : A
",
37,"An online reseller has a large, multi-column dataset with one column missing 30% of its data A Machine Learning Specialist believes that certain columns in the dataset could be used to reconstruct the missing data
Which reconstruction approach should the Specialist use to preserve the integrity of the dataset?

A. Listwise deletion

B. Last observation carried forward

C. Multiple imputation

D. Mean substitution
","Answers : C
",
38,"A Machine Learning Specialist is using Amazon SageMaker to host a model for a highly available customer-facing application .
The Specialist has trained a new version of the model, validated it with historical data, and now wants to deploy it to production To limit any risk of a negative customer experience, the Specialist wants to be able to monitor the model and roll it back, if needed 
What is the SIMPLEST approach with the LEAST risk to deploy the model and roll it back, if needed?

A. Create a SageMaker endpoint and configuration for the new model version. Redirect production traffic to the new endpoint by updating the client configuration. Revert traffic to the last version if the model does not perform as expected.

B. Create a SageMaker endpoint and configuration for the new model version. Redirect production traffic to the new endpoint by using a load balancer Revert traffic to the last version if the model does not perform as expected.

C. Update the existing SageMaker endpoint to use a new configuration that is weighted to send 5% of the traffic to the new variant. Revert traffic to the last version by resetting the weights if the model does not perform as expected.

D. Update the existing SageMaker endpoint to use a new configuration that is weighted to send 100% of the traffic to the new variant Revert traffic to the last version by resetting the weights if the model does not perform as expected.
","Answers : A
",
39,"A manufacturing company asks its Machine Learning Specialist to develop a model that classifies defective parts into one of eight defect types. The company has provided roughly 100000 images per defect type for training During the injial training of the image  classification model the Specialist notices that the validation accuracy is 80%, while the training accuracy is 90% It is known that human-level performance for this type of image classification is around 90%
What should the Specialist consider to fix this issue1?

A. A longer training time

B. Making the network larger

C. Using a different optimizer

D. Using some form of regularization
","Answers : D
",
40,"A Machine Learning Specialist works for a credit card processing company and needs to predict which transactions may be fraudulent in near-real time.
Specifically, the Specialist must train a model that returns the probability that a given transaction may be fraudulent
How should the Specialist frame this business problem?

A. Streaming classification

B. Binary classification

C. Multi-category classification

D. Regression classification
","Answers : C
",
41,"A Machine Learning Specialist built an image classification deep learning model. However the Specialist ran into an overfitting problem in which the training and testing accuracies were 99% and 75%r respectively.
How should the Specialist address this issue and what is the reason behind it?

A. The learning rate should be increased because the optimization process was trapped at a local minimum.

B. The dropout rate at the flatten layer should be increased because the model is not generalized enough.

C. The dimensionality of dense layer next to the flatten layer should be increased because the model is not complex enough.

D. The epoch number should be increased because the optimization process was terminated before it reached the global minimum.
","Answers : D
",
42,"Example Corp has an annual sale event from October to December. The company has sequential sales data from the past 15 years and wants to use Amazon ML to predict the sales for this year's upcoming event. Which method should Example Corp use to split the data into a training dataset and evaluation dataset?

A. Pre-split the data before uploading to Amazon S3

B. Have Amazon ML split the data randomly.

C. Have Amazon ML split the data sequentially.

D. Perform custom cross-validation on the data
","Answers : C
",
43,"A Machine Learning Specialist wants to determine the appropriate SageMaker Variant Invocations Per Instance setting for an endpoint automatic scaling configuration. The Specialist has performed a load test on a single instance and determined that peak requests per second (RPS) without service degradation is about 20 RPS As this is the first deployment, the Specialist intends to set the invocation safety factor to 0.5
Based on the stated parameters and given that the invocations per instance setting is measured on a per-minute basis, what should the Specialist set as the sageMakervariantinvocationsPerinstance setting?

A. 10

B. 30

C. 600

D. 2,400
","Answers : C

SageMakerVariantInvocationsPerInstance 
= (MAX_RPS * SAFETY_FACTOR) * 60
= (0.5 * 20) * 60
= 600
AWS recommended safety factor = 0 .5

https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-scaling-loadtest.html",
44,"A Machine Learning Specialist deployed a model that provides product recommendations on a company's website Initially, the model was performing very well and resulted in customers buying more products on average However within the past few months the Specialist has noticed that the effect of product recommendations  has diminished and customers are starting to return to their original habits of spending less The Specialist is unsure of what happened, as the model has not changed from its initial deployment over a year ago.
Which method should the Specialist try to improve model performance?

A. The model needs to be completely re-engineered because it is unable to handle product inventory changes

B. The model's hyperparameters should be periodically updated to prevent drift

C. The model should be periodically retrained from scratch using the original data while adding a regularization term to handle product inventory changes

D. The model should be periodically retrained using the original training data plus new data as product inventory changes
","Answers : D
",
45,"The Chief Editor for a product catalog wants the Research and Development team to build a machine learning system that can be used to detect whether or not individuals in a collection of images are wearing the company's retail brand The team has a set of training data
Which machine learning algorithm should the researchers use that BEST meets their requirements?

A. Latent Dirichlet Allocation (LDA)

B. Recurrent neural network (RNN)

C. K-means

D. Convolutional neural network (CNN)
","Answers : C
",
46,"A Machine Learning Specialist working for an online fashion company wants to build a data ingestion solution for the company's Amazon S3-based data lake. The Specialist wants to create a set of ingestion mechanisms that will enable future capabilities comprised of:
• Real-time analytics
• Interactive analytics of historical data
• Clickstream analytics
• Product recommendations
Which services should the Specialist use?

A. AWS Glue as the data dialog; Amazon Kinesis Data Streams and Amazon Kinesis Data Analytics for real-time data insights; Amazon Kinesis Data Firehose for delivery to Amazon ES for clickstream analytics; Amazon EMR to generate personalized product recommendations

B. Amazon Athena as the data catalog; Amazon Kinesis Data Streams and Amazon Kinesis Data Analytics for near-realtime data insights; Amazon Kinesis Data Firehose for clickstream analytics; AWS Glue to generate personalized product recommendations

C. AWS Glue as the data catalog; Amazon Kinesis Data Streams and Amazon Kinesis Data Analytics for historical data insights; Amazon Kinesis Data Firehose for delivery to Amazon ES for clickstream analytics; Amazon EMR to generate personalized product recommendations

D. Amazon Athena as the data catalog; Amazon Kinesis Data Streams and Amazon Kinesis Data Analytics for historical data insights; Amazon DynamoDB streams for clickstream analytics; AWS Glue to generate personalized product recommendations
","Answers : A
",
47,"A Machine Learning Specialist is designing a system for improving sales for a company. The objective is to use the large amount of information the company has on users' behavior and product preferences to predict which products users would like based on the users' similarity to other users.
What should the Specialist do to meet this objective?

A. Build a content-based filtering recommendation engine with Apache Spark ML on Amazon EMR.

B. Build a collaborative filtering recommendation engine with Apache Spark ML on Amazon EMR.

C. Build a model-based filtering recommendation engine with Apache Spark ML on Amazon EMR.

D. Build a combinative filtering recommendation engine with Apache Spark ML on Amazon EMR.

","Answer: B
 
Many developers want to implement the famous Amazon model that was used to power the “People who bought this also bought these items” feature on Amazon.com. This model is based on a method called Collaborative Filtering. It takes items such as movies, books, and products that were rated highly by a set of users and recommending them to other users who also gave them high ratings. This method works well in domains where explicit ratings or implicit user actions can be gathered and analyzed.
",
48,"A Machine Learning Specialist is packaging a custom ResNet model into a Docker container so the company can leverage Amazon SageMaker for training The Specialist is using Amazon EC2 P3 instances to train the model and needs to properly configure the Docker container to leverage the NVIDIA GPUs
What does the Specialist need to do1?

A. Bundle the NVIDIA drivers with the Docker image

B. Build the Docker container to be NVIDIA-Docker compatible

C. Organize the Docker container's file structure to execute on GPU instances.

D. Set the GPU flag in the Amazon SageMaker Create TrainingJob request body
","Answers : A
",
49,"A Machine Learning Specialist is using Apache Spark for pre-processing training data As part of the Spark pipeline, the Specialist wants to use Amazon SageMaker for training a model and hosting it Which of the following would the Specialist do to integrate the Spark application with SageMaker? (Select THREE )

A. Download the AWS SDK for the Spark environment

B. Install the SageMaker Spark library in the Spark environment.

C. Use the appropriate estimator from the SageMaker Spark Library to train a model.

D. Compress the training data into a ZIP file and upload it to a pre-defined Amazon S3 bucket.

E. Use the SageMaker Mode

F. transform method to get inferences from the model hosted in SageMaker

G. Convert the DataFrame object to a CSV file, and use the CSV file as input for obtaining inferences from SageMaker.
","Answers : D,E,F
",
50,"A Machine Learning Specialist is configuring automatic model tuning in Amazon SageMaker
When using the hyperparameter optimization feature, which of the following guidelines should be followed to improve optimization?

Choose the maximum number of hyperparameters supported by

A. Amazon SageMaker to search the largest number of combinations possible

B. Specify a very large hyperparameter range to allow Amazon SageMaker to cover every possible value.

C. Use log-scaled hyperparameters to allow the hyperparameter space to be searched as quickly as possible

D. Execute only one hyperparameter tuning job at a time and improve tuning through successive rounds of experiments
","Answers : C
",
51,"A monitoring service generates 1 TB of scale metrics record data every minute A Research team performs queries on this data using Amazon Athena The queries run slowly due to the large volume of data, and the team requires better performance
How should the records be stored in Amazon S3 to improve query performance?

A. CSV files

B. Parquet files

C. Compressed JSON

D. RecordIO
","Answers : B
",
52,"A Machine Learning Specialist must build out a process to query a dataset on Amazon S3 using Amazon Athena. The dataset contains more than 800.000 records stored as plaintext CSV files Each record contains 200 columns and is approximately 1 5 MB in size Most queries will span 5 to 10 columns only.
How should the Machine Learning Specialist transform the dataset to minimize query runtime?

A. Convert the records to Apache Parquet format

B. Convert the records to JSON format

C. Convert the records to GZIP CSV format

D. Convert the records to XML format
","Answers : A
",
53,"A Machine Learning Specialist trained a regression model, but the first iteration needs optimizing. The Specialist needs to understand whether the model is more frequently overestimating or underestimating the target.
What option can the Specialist use to determine whether it is overestimating or underestimating the target value?

A. Root Mean Square Error (RMSE)

B. Residual plots

C. Area under the curve

D. Confusion matrix
","Answers : C
",
54,"A Data Engineer needs to build a model using a dataset containing customer credit card information.
How can the Data Engineer ensure the data remains encrypted and the credit card information is secure? Use a custom encryption algorithm to encrypt the data and store the data on an Amazon SageMaker instance in a VPC. Use the SageMaker DeepAR  algorithm to randomize the credit card numbers.

A. Use an IAM policy to encrypt the data on the Amazon S3 bucket and Amazon Kinesis to automatically discard credit card numbers and insert fake credit card numbers.

B. Use an Amazon SageMaker launch configuration to encrypt the data once it is copied to the SageMaker instance in a VP

C. Use the SageMaker principal component analysis (PCA) algorithm to reduce the length of the credit card numbers.

D. Use AWS KMS to encrypt the data on Amazon S3
","Answers : C
",
55,"An insurance company is developing a new device for vehicles that uses a camera to observe drivers' behavior and alert them when they appear distracted The company created approximately 10,000 training images in a controlled environment that a Machine Learning Specialist will use to train and evaluate machine learning models
During the model evaluation the Specialist notices that the training error rate diminishes faster as the number of epochs increases and the model is not accurately inferring on the unseen test images
Which of the following should be used to resolve this issue? (Select TWO)

A. Add vanishing gradient to the model

B. Perform data augmentation on the training data

C. Make the neural network architecture complex.

D. Use gradient checking in the model

E. Add L2 regularization to the model
","Answers : B,D
",
56,"A company is setting up an Amazon SageMaker environment. The corporate data security policy does not allow communication over the internet.
How can the company enable the Amazon SageMaker service without enabling direct internet access to Amazon SageMaker notebook instances?

A. Create a NAT gateway within the corporate VPC.

B. Route Amazon SageMaker traffic through an on-premises network.

C. Create Amazon SageMaker VPC interface endpoints within the corporate VPC.

D. Create VPC peering with Amazon VPC hosting Amazon SageMaker.
","Answers : A
",
57,"A retail company intends to use machine learning to categorize new products A labeled dataset of current products was provided to the Data Science team The dataset includes 1 200 products The labeled dataset has 15 features for each product such as title dimensions, weight, and price Each product is labeled as belonging to one of six categories such as books, games, electronics, and movies.
Which model should be used for categorizing new products using the provided dataset for training?

A. An XGBoost model where the objective parameter is set to multi: softmax

B. A deep convolutional neural network (CNN) with a softmax activation function for the last layer

C. A regression forest where the number of trees is set equal to the number of product categories

D. A DeepAR forecasting model based on a recurrent neural network (RNN)
","Answers : B
",
58,"A Machine Learning Specialist is building a logistic regression model that will predict whether or not a person will order a pizza. The Specialist is trying to build the optimal model with an ideal classification threshold.
What model evaluation technique should the Specialist use to understand how different classification thresholds will impact the model's performance?

A. Receiver operating characteristic (ROC) curve

B. Misclassification rate

C. Root Mean Square Error (RMSE)

D. L1 norm
","Answers : A
",
59,"A Machine Learning Specialist is developing a daily ETL workflow containing multiple ETL jobs The workflow consists of the following processes
* Start the workflow as soon as data is uploaded to Amazon S3
* When all the datasets are available in Amazon S3, start an ETL job to join the  uploaded datasets with multiple terabyte-sized datasets already stored in Amazon S3
* Store the results of joining datasets in Amazon S3
* If one of the jobs fails, send a notification to the Administrator Which configuration will meet these requirements?

A. Use AWS Lambda to trigger an AWS Step Functions workflow to wait for dataset uploads to complete in Amazon S3. Use AWS Glue to join the datasets Use an Amazon CloudWatch alarm to send an SNS notification to the Administrator in the case of a failure

B. Develop the ETL workflow using AWS Lambda to start an Amazon SageMaker notebook instance Use a lifecycle configuration script to join the datasets and persist the results in Amazon S3 Use an Amazon CloudWatch alarm to send an SNS notification to the Administrator in the case of a failure

C. Develop the ETL workflow using AWS Batch to trigger the start of ETL jobs when data is uploaded to Amazon S3 Use AWS Glue to join the datasets in Amazon S3 Use an Amazon CloudWatch alarm to send an SNS notification to the Administrator in the case of a failure

D. Use AWS Lambda to chain other Lambda functions to read and join the datasets in Amazon S3 as soon as the data is uploaded to Amazon S3 Use an Amazon CloudWatch alarm to send an SNS notification to the Administrator in the case of a failure
","Answers : A
",
60,"Amazon Connect has recently been tolled out across a company as a contact call center The solution has been configured to store voice call recordings on Amazon S3
The content of the voice calls are being analyzed for the incidents being discussed by the call operators Amazon Transcribe is being used to convert the audio to text, and the output is stored on Amazon S3
Which approach will provide the information required for further analysis?

A. Use Amazon Comprehend with the transcribed files to build the key topics

B. Use Amazon Translate with the transcribed files to train and build a model for the key topics

C. Use the AWS Deep Learning AMI with Gluon Semantic Segmentation on the transcribed files to train and build a model for the key topics

D. Use the Amazon SageMaker k-Nearest-Neighbors (kNN) algorithm on the transcribed files to generate a word embeddings dictionary for the key topics
","Answers : B
",
61,"A company is observing low accuracy while training on the default built-in image classification algorithm in Amazon SageMaker. The Data Science team wants to use an Inception neural network architecture instead of a ResNet architecture.
Which of the following will accomplish this? (Select TWO.)

A. Customize the built-in image classification algorithm to use Inception and use this for model training.

B. Create a support case with the SageMaker team to change the default image classification algorithm to Inception.

C. Bundle a Docker container with TensorFlow Estimator loaded with an Inception network and use this for model training.

D. Use custom code in Amazon SageMaker with TensorFlow Estimator to load the model with an Inception network and use this for model training.

E. Download and apt-get install the inception network code into an Amazon EC2 instance and use this instance as a Jupyter notebook in Amazon SageMaker.
","Answers : A,D
",
62,"A Machine Learning Specialist needs to create a data repository to hold a large amount of time-based training data for a new model. In the source system, new files are added every hour Throughout a single 24-hour period, the volume of hourly updates will change significantly. The Specialist always wants to train on the last 24 hours of the data
Which type of data repository is the MOST cost-effective solution?

A. An Amazon EBS-backed Amazon EC2 instance with hourly directories

B. An Amazon RDS database with hourly table partitions

C. An Amazon S3 data lake with hourly object prefixes

D. An Amazon EMR cluster with hourly hive partitions on Amazon EBS volumes
","Answers : C
",
63,"A Machine Learning Specialist at a company sensitive to security is preparing a dataset for model training. The dataset is stored in Amazon S3 and contains Personally Identifiable Information (Pll). The dataset:
* Must be accessible from a VPC only.
* Must not traverse the public internet. How can these requirements be satisfied?

A. Create a VPC endpoint and apply a bucket access policy that restricts access to the given VPC endpoint and the VPC.

B. Create a VPC endpoint and apply a bucket access policy that allows access from the given VPC endpoint and an Amazon EC2 instance.

C. Create a VPC endpoint and use Network Access Control Lists (NACLs) to allow traffic between only the given VPC endpoint and an Amazon EC2 instance.

D. Create a VPC endpoint and use security groups to restrict access to the given VPC endpoint and an Amazon EC2 instance.
","Answers : B

keyword : bucket

",
64,"A Data Science team within a large company uses Amazon SageMaker notebooks to access data stored in Amazon S3 buckets. The IT Security team is concerned that internet-enabled notebook instances create a security vulnerability where malicious code running on the instances could compromise data privacy.
The company mandates that all instances stay within a secured VPC with no internet access, and data communication traffic must stay within the AWS network.
How should the Data Science team configure the notebook instance placement to meet these requirements?

A. Associate the Amazon SageMaker notebook with a private subnet in a VP

B. Place the Amazon SageMaker endpoint and S3 buckets within the same VPC.

C. Associate the Amazon SageMaker notebook with a private subnet in a VP

D. Use IAM policies to grant access to Amazon S3 and Amazon SageMaker.

E. Associate the Amazon SageMaker notebook with a private subnet in a VP

F. Ensure the VPC has S3 VPC endpoints and Amazon SageMaker VPC endpoints attached to it.

G. Associate the Amazon SageMaker notebook with a private subnet in a VP

H. Ensure the VPC has a NAT gateway and an associated security group allowing only outbound connections to Amazon S3 and Amazon SageMaker
","Answers : D
",
65,"A Machine Learning Specialist needs to move and transform data in preparation for training Some of the data needs to be processed in near-real time and other data can be moved hourly There are existing Amazon EMR MapReduce jobs to clean and feature engineering to perform on the data
Which of the following services can feed data to the MapReduce jobs? (Select TWO )

A. AWS Database Migration Service (DMS) 

B. Amazon Kinesis

C. AWS Data Pipeline

D. Amazon Athena

E. Amazon ES
","Answers : B,D

Athena: run SQL queries on files in S3. Big Data tool for OLAP (Online Analytical Processing). Example use case: To store gigabytes of data in S3 as files (for example parquet, but also JSON, CSV, etc.) and to run a query on it. 

Kinesis Analytics: run SQL queries on a data stream. It may be a windowed query. 
Example use case: with a stream of e-store transactions, get the transaction count or summary value over the last hour. The output can be produced every transaction or every minute, so the output is a new stream of (summary) values.",
66,"A Machine Learning Specialist is developing recommendation engine for a photography blog Given a picture, the recommendation engine should show a picture that captures similar objects. The Specialist would like to create a numerical representation feature to perform nearest-neighbor searches What actions would allow the Specialist to get relevant numerical representations?

A. Reduce image resolution and use reduced resolution pixel values as features

B. Use Amazon Mechanical Turk to label image content and create a one-hot representation indicating the presence of specific labels

C. Run images through a neural network pie-trained on ImageNet, and collect the feature vectors from the penultimate layer

D. Average colors by channel to obtain three-dimensional representations of images.
","Answers : A
",
67,"An office security agency conducted a successful pilot using 100 cameras installed at key locations within the main office. Images from the cameras were uploaded to Amazon S3 and tagged using Amazon Rekognition, and the results were stored in Amazon ES. The agency is now looking to expand the pilot into a full production system using thousands of video cameras in its office locations globally. The goal is to identify activities performed by non-employees in real time.
Which solution should the agency consider?

A. Use a proxy server at each local office and for each camera, and stream the RTSP feed to a unique Amazon Kinesis Video Streams video strea

B. On each stream, use Amazon Rekognition Video and createa stream processor to detect faces from a collection of known employees, and alert when non employees are detected.

C. Use a proxy server at each local office and for each camera, and stream the RTSP feed to a unique Amazon Kinesis Video Streams video stream

D. On each stream, use Amazon Rekognition Image to detect faces from a collection of known employees and alert when non-employees are detected.

E. Install AWS DeepLens cameras and use the DeepLens_Kinesis_Video module to stream video to Amazon Kinesis Video Streams for each camera

F. On each stream, use Amazon Rekognition Video and create a stream processor to detect faces from a collection on each stream, and alert when non employees are detected.

G. Install AWS DeepLens cameras and use the DeepLens_Kinesis_Video module to stream video to Amazon Kinesis Video Streams for each camera

H. On each stream, run an AWS Lambda function to capture image fragments and then call Amazon Rekognition Image to detect faces from a collection of known employees, and alert when non-employees are detected.
","Answers : D
",
68,"A Machine Learning Specialist prepared the following graph displaying the results of k-means for k = [1:10]
Considering the graph, what is a reasonable selection for the optimal choice of k?

A. 1

B. 4

C. 7

D. 10
","Answers : B

k = 4
https://en.wikipedia.org/wiki/Elbow_method_(clustering)",qn68.jpg
69,"A Marketing Manager at a pet insurance company plans to launch a targeted marketing campaign on social media to acquire new customers. Currently, the company has the following data in Amazon Aurora
• Profiles for all past and existing customers
• Profiles for all past and existing insured pets
• Policy-level information
• Premiums received
• Claims paid
What steps should be taken to implement a machine learning model to identify potential new customers on social media?

A. Use regression on customer profile data to understand key characteristics of consumer segments Find similar profiles on social media.

B. Use clustering on customer profile data to understand key characteristics of consumer segments Find similar profiles on social media.

C. Use a recommendation engine on customer profile data to understand key characteristics of consumer segment

D. Use a decision tree classifier engine on customer profile data to understand key characteristics of consumer segment
","Answers : C
",
70,"During mini-batch training of a neural network for a classification problem, a Data Scientist notices that training accuracy oscillates What is the MOST likely cause of this issue?

A. The class distribution in the dataset is imbalanced

B. Dataset shuffling is disabled

C. The batch size is too big

D. The learning rate is very high
","Answers : D
",
71,"A Machine Learning Specialist is building a convolutional neural network (CNN) that will classify 10 types of animals. The Specialist has built a series of layers in a neural network that will take an input image of an animal, pass it through a series of convolutional and pooling layers, and then finally pass it through a dense and fully connected layer with 10 nodes The Specialist would like to get an output from the neural network that is a probability distribution of how likely it is that the input image belongs to each of the 10 classes
Which function will produce the desired output?

A. Dropout

B. Smooth L1 loss

C. Softmax

D. Rectified linear units (ReLU)

","Answers : D

Dropout layers helps to prevent overfitting
ReLU is an activation function",
72,"A company's Machine Learning Specialist needs to improve the training speed of a time-series forecasting model using TensorFlow. The training is currently implemented on a single-GPU machine and takes approximately 23 hours to complete. The training needs to be run daily.
The model accuracy is acceptable, but the company anticipates a continuous increase in the size of the training data and a need to update the model on an hourly, rather than a daily, basis. The company also wants to minimize coding effort and infrastructure changes
What should the Machine Learning Specialist do to the training solution to allow it to scale for future demand?

A. Do not change the TensorFlow cod

B. Change the machine to one with a more powerful GPU to speed up the training.

C. Change the TensorFlow code to implement a Horovod distributed framework supported by Amazon SageMaker

D. Parallelize the training to as many machines as needed to achieve the business goals.

E. Switch to using a built-in AWS SageMaker DeepAR mode

F. Parallelize the training to as many machines as needed to achieve the business goals.

G. Move the training to Amazon EMR and distribute the workload to as many machines as needed to achieve the business goals.
","Answers : B
",
73,"A Machine Learning Specialist is working with a large company to leverage machine learning within its products. The company wants to group its customers into categories based on which customers will and will not churn within the next 6 months. The company has labeled the data available to the Specialist.
Which machine learning model type should the Specialist use to accomplish this task?

A. Linear regression

B. Classification

C. Clustering

D. Reinforcement learning
","Answers : B
   
The goal of classification is to determine to which class or category a data point (customer in our case) belongs to. For classification problems, data scientists
would use historical data with predefined target variables AKA labels (churner/non-churner) – answers that need to be predicted – to train an algorithm. With
classification,
businesses can answer the following questions:
Will this customer churn or not?
Will a customer renew their subscription?
Will a user downgrade a pricing plan?
Are there any signs of unusual customer behavior?",
74,"A Machine Learning Specialist is building a model to predict future employment rates based on a wide range of economic factors While exploring the data, the Specialist notices that the magnitude of the input features vary greatly The Specialist does not want variables with a larger magnitude to dominate the model
What should the Specialist do to prepare the data for model training'?

A. Apply quantile binning to group the data into categorical bins to keep any  relationships in the data by replacing the magnitude with distribution

B. Apply the Cartesian product transformation to create new combinations of fields that are independent of the magnitude

C. Apply normalization to ensure each field will have a mean of 0 and a variance of 1 to remove any significant magnitude

D. Apply the orthogonal sparse Diagram (OSB) transformation to apply a fixed-size sliding window to generate new features of a similar magnitude.
","Answers : C
",
75,"A Machine Learning Specialist is building a prediction model for a large number of features using linear models, such as linear regression and logistic regression
During exploratory data analysis the Specialist observes that many features are highly correlated with each other This may make the model unstable
What should be done to reduce the impact of having such a large number of features?

A. Perform one-hot encoding on highly correlated features

B. Use matrix multiplication on highly correlated features.

C. Create a new feature space using principal component analysis (PCA)

D. Apply the Pearson correlation coefficient
","Answers : C
",
76,"A Machine Learning Specialist is building a supervised model that will evaluate customers' satisfaction with their mobile phone service based on recent usage The model's output should infer whether or not a customer is likely to switch to a competitor in the next 30 days
Which of the following modeling techniques should the Specialist use1?

A. Time-series prediction

B. Anomaly detection

C. Binary classification

D. Regression
","Answers : D
",
77,"A Data Scientist wants to gain real-time insights into a data stream of GZIP files. Which solution would allow the use of SQL to query the stream with the LEAST latency?

A. Amazon Kinesis Data Analytics with an AWS Lambda function to transform the data.

B. AWS Glue with a custom ETL script to transform the data.

C. An Amazon Kinesis Client Library to transform the data and save it to an Amazon ES cluster.

D. Amazon Kinesis Data Firehose to transform the data and put it into an Amazon S3 bucket.
","Answers : A
",
78,"A Machine Learning Specialist is building a model that will perform time series forecasting using Amazon SageMaker. The Specialist has finished training the model and is now planning to perform load testing on the endpoint so they can configure Auto Scaling for the model variant.
Which approach will allow the Specialist to review the latency, memory utilization, and CPU utilization during the load test?
A. Review SageMaker logs that have been written to Amazon S3 by leveraging Amazon Athena and Amazon QuickSight to visualize logs as they are being produced.

B. Generate an Amazon CloudWatch dashboard to create a single view for the latency, memory utilization, and CPU utilization metrics that are outputted by Amazon SageMaker.

C. Build custom Amazon CloudWatch Logs and then leverage Amazon ES and Kibana to query and visualize the log data as it is generated by Amazon SageMaker.

D. Send Amazon CloudWatch Logs that were generated by Amazon SageMaker to Amazon ES and use Kibana to query and visualize the log data.
","Answers : B
",
79,"A Machine Learning Specialist kicks off a hyperparameter tuning job for a tree-based ensemble model using Amazon SageMaker with Area Under the ROC Curve (AUC) as the objective metric. This workflow will eventually be deployed in a pipeline that retrains and tunes hyperparameters each night to model click-through on data that goes stale every 24 hours.
With the goal of decreasing the amount of time it takes to train these models, and ultimately to decrease costs, the Specialist wants to reconfigure the input hyperparameter range(s).
Which visualization will accomplish this?

A. A histogram showing whether the most important input feature is Gaussian.

B. A scatter plot with points colored by target variable that uses t-Distributed Stochastic Neighbor Embedding (t-SNE) to visualize the large number of input variables in an easier-to-read dimension.

C. A scatter plot showing the performance of the objective metric over each training iteration.

D. A scatter plot showing the correlation between maximum tree depth and the objective metric.
","Answers : D

A and B were about data discovery
",
80,"A web-based company wants to improve its conversion rate on its landing page Using a large historical dataset of customer visits, the company has repeatedly trained a multi-class deep learning network algorithm on Amazon SageMaker However there is an overfitting problem training data shows 90% accuracy in predictions, while test data shows 70% accuracy only
The company needs to boost the generalization of its model before deploying it into production to maximize conversions of visits to purchases
Which action is recommended to provide the HIGHEST accuracy model for the company's test and validation data?

A. Increase the randomization of training data in the mini-batches used in training.

B. Allocate a higher proportion of the overall data to the training dataset

C. Apply L1 or L2 regularization and dropouts to the training.

D. Reduce the number of layers and units (or neurons) from the deep learning network.
","Answers : A
",
81,"A Data Scientist needs to create a serverless ingestion and analytics solution for high-velocity, real-time streaming data.
The ingestion process must buffer and convert incoming records from JSON to a query-optimized, columnar format without data loss. The output datastore must be highly available, and Analysts must be able to run SQL queries against the data and connect to existing business intelligence dashboards.
Which solution should the Data Scientist build to satisfy the requirements?

A. Create a schema in the AWS Glue Data Catalog of the incoming data forma

B. Use an Amazon Kinesis Data Firehose delivery stream to stream the data and transform the data to Apache Parquet or ORC format using the AWS Glue Data Catalog before delivering to Amazon S3. Have the Analysts query the data directly from Amazon S3 using Amazon Athena, and connect to Bl tools using the Athena Java Database Connectivity (JDBC) connector.

C. Write each JSON record to a staging location in Amazon S3. Use the S3 Put event to trigger an AWS Lambda function that transforms the data into Apache Parquet or ORC format and writes the data to a processed data location in Amazon S3. Have the Analysts query the data directly from Amazon S3 using Amazon Athena, and connect to Bl tools using the Athena Java Database Connectivity (JDBC) connector.

D. Write each JSON record to a staging location in Amazon S3. Use the S3 Put event to trigger an AWS Lambda function that transforms the data into Apache Parquet or ORC format and inserts it into an Amazon RDS PostgreSQL database

E. Have the Analysts query and run dashboards from the RDS database.

F. Use Amazon Kinesis Data Analytics to ingest the streaming data and perform real-time SQL queries to convert the records to Apache Parquet before delivering to Amazon S3. Have the Analysts query the data directly from Amazon S3 using Amazon Athena and connect to Bl tools using the Athena Java Database Connectivity (JDBC) connector.
","Answers : A

A. YES - we need a catalog to create parquet 
B. NO - no need for extra staging
C. NO - RDS ISN'T serverless ingestion and analytics solution
D. NO - Kinesis Data Analytics NO PARQET FORMAT
",
82,"A Machine Learning Specialist is training a model to identify the make and model of vehicles in images. The Specialist wants to use transfer learning and an existing model trained on images of general objects. The Specialist collated a large custom dataset of pictures containing different vehicle makes and models.
What should the Specialist do to initialize the model to re-train it with the custom data?

A. Initialize the model with random weights in all layers including the last fully connected layer.

B. Initialize the model with pre-trained weights in all layers and replace the last fully connected layer.

C. Initialize the model with random weights in all layers and replace the last fully connected layer.

D. Initialize the model with pre-trained weights in all layers including the last fully connected layer.
","Answers : B

A. NO - random weights does not allow transfer learning
B. YES - the last layer gives the final classes, we want to have new classes
C. NO - random weights does not allow transfer learning
D. NO - the last layer gives the final classes, we want to have new classes

In transfer learning, a pre-trained model is used as a starting point to train a new model on a different task, typically using a smaller dataset. 
The pre-trained model contains weights that have been learned from a large amount of data on a related task, and these weights can be leveraged to train the new model more efficiently.

To re-train the model with the custom data, the Specialist should initialize the model with pre-trained weights in all layers, as these weights can provide a good starting point for the new task. 
The Specialist should then replace the last fully connected layer, which is responsible for making the final predictions, as this layer will likely need to be modified to reflect the new task. 
By keeping the pre-trained weights in the other layers, the Specialist can take advantage of the knowledge learned from the previous task, and potentially speed up the training process.
",
83,"An Amazon SageMaker notebook instance is launched into Amazon VPC The SageMaker notebook references data contained in an Amazon S3 bucket in another account The bucket is encrypted using SSE-KMS The instance returns an access denied error when trying to access data in Amazon S3.
Which of the following are required to access the bucket and avoid the access denied error? (Select THREE )

A. An AWS KMS key policy that allows access to the customer master key (CMK)

B. A SageMaker notebook security group that allows access to Amazon S3

C. An IAM role that allows access to the specific S3 bucket

D. A permissive S3 bucket policy

E. An S3 bucket owner that matches the notebook owner

F. A SegaMaker notebook subnet ACL that allow traffic to Amazon S3.
","Answers : A,C,F
",
84,"A Machine Learning Specialist is assigned a TensorFlow project using Amazon SageMaker for training, and needs to continue working for an extended period with no Wi-Fi access.
Which approach should the Specialist use to continue working?

A. Install Python 3 and boto3 on their laptop and continue the code development using that environment.

B. Download the TensorFlow Docker container used in Amazon SageMaker from GitHub to their local environment, and use the Amazon SageMaker Python SDK to test the code.

C. Download TensorFlow from tensorflow.org to emulate the TensorFlow kernel in the SageMaker environment.

D. Download the SageMaker notebook to their local environment then install Jupyter Notebooks on their laptop and continue the development in a local notebook.
","Answers : A
",
85,"A Machine Learning Engineer wants to use Amazon SageMaker and the built-in XGBoost algorithm for model training. The training data is currently stored in CSV format, with the first 10 columns representing features and the 11th column representing the target label. What should the ML Engineer do to prepare the data for use in an Amazon SageMaker training job?

A.
The target label should be changed to the first column. The data should be split into training, validation, and test sets. Finally, the datasets should be uploaded to Amazon S3

B.
The dataset should be uploaded directly to Amazon S3. Amazon SageMaker can then be used to split the data into training, validation, and test sets.

C.
The data should be split into training, validation, and test sets. The datasets should then be uploaded to Amazon S3.

D.
The target label should be changed to the first column. The dataset should then be uploaded to Amazon S3. Finally, Amazon SageMaker can be used to split the data into training, validation, and test sets.

","Correct Answer
A. The target label should be changed to the first column. The data should be split into training, validation, and test sets. Finally, the datasets should be uploaded to Amazon S3

Explanation
To prepare the data for use in an Amazon SageMaker training job, the ML Engineer should first change the target
label to the first column. Then, the data should be split into training, validation, and test sets. Finally, the datasets
should be uploaded to Amazon S3.",
86,"A Machine Learning Specialist was given a dataset consisting of unlabeled data The Specialist must create a model that can help the team classify the data into different buckets What model should be used to complete this work?

A.
K-means clustering

B.
Random Cut Forest (RCF)

C.
XGBoost

D.
BlazingText
","Correct Answer
A. K-means clustering
Explanation
K-means clustering should be used to complete this work because it is a popular unsupervised learning algorithm that is used for clustering data. It is suitable for this task because the dataset consists of unlabeled data and the goal is to classify the data into different buckets. K-means clustering works by partitioning the data into k clusters based on their similarity. It iteratively assigns data points to the nearest cluster centroid and updates the centroids until convergence. This algorithm is widely used for data clustering and can help the Machine Learning Specialist in this task.",
87,"A large JSON dataset for a project has been uploaded to a private Amazon S3 bucket. The Machine Learning Specialist wants to securely access and explore the data from an Amazon SageMaker notebook instance. A new VPC was created and assigned to the Specialist. How can the privacy and integrity of the data stored in Amazon S3 be
maintained while granting access to the Specialist for analysis?

A.
Launch the SageMaker notebook instance within the VPC with SageMaker-provided internet access enabled. Use an S3 ACL to open read privileges to the everyone group

B.
Launch the SageMaker notebook instance within the VPC and create an S3 VPC endpoint for the notebook to access the data. Copy the JSON dataset from Amazon S3 into the ML storage volume on the SageMaker notebook instance and work against the local dataset.

C.
Launch the SageMaker notebook instance within the VPC and create an S3 VPC endpoint for the notebook to access the data, define a custom S3 bucket policy to only allow requests from your VPC to access the S3 bucket.

D.
Launch the SageMaker notebook instance within the VPC with SageMaker-provided internet access enabled. Generate an S3 pre-signed URL for access to data in the bucket.
","Correct Answer
C. Launch the SageMaker notebook instance within the VPC and create an S3 VPC endpoint for the notebook to
access the data, define a custom S3 bucket policy to only allow requests from your VPC to access the S3 bucket.
Explanation
The correct answer is to launch the SageMaker notebook instance within the VPC and create an S3 VPC endpoint for
the notebook to access the data, and define a custom S3 bucket policy to only allow requests from the VPC to access
the S3 bucket. This ensures that the data stored in Amazon S3 remains private and can only be accessed by the
Specialist through the VPC and the designated notebook instance. The S3 VPC endpoint establishes a private
connection between the VPC and S3, eliminating the need for internet access. The custom S3 bucket policy further
restricts access to the bucket, ensuring the integrity of the data.",
88,"You work in the security department of your company's IT division. Your company has decided to try to use facial recognition to improve security on their campus. You have been asked to design a system that augments your company's building access security by scanning the faces of people entering their buildings and recognizing the person as either an employee/contractor/consultant, who is in the company's database, or  visitor, who is not in their database. Across their many campus locations worldwide your company has over 750,000 employees and over 250,000 contractors and consultants. These workers are all registered in their HR database. Each of these workers has an image of their face stored in the HR database. You have decided to use Amazon Rekognition for your facial recognition solution. On occasion, the Rekognition model fails to recognize visitors to the buildings. What could be the source of the problem?

A.
Face landmarks filters set to a max sharpness

B.
Bounding box and confidence score for face comparison threshold tolerances set to max values

C.
Confidence threshold tolerance set to the default

D.
Face collection contents
","Correct Answer
D. Face collection contents
Explanation
The source of the problem could be the face collection contents. Since the Rekognition model is failing to recognize visitors, it is possible that the faces of the visitors are not included in the face collection that the system is comparing against. The face collection should ideally contain images of both employees/contractors/consultants and visitors in order to accurately identify and differentiate between them.",
89,"A city wants to monitor its air quality to address the consequences of air pollution. A Machine Learning Specialist needs to forecast the air quality in parts per million of contaminates for the next 2 days in the city. As this is a prototype, only daily data from the last year is available. Which model is MOST likely to provide the best results in
Amazon SageMaker?

A.
Use the Amazon SageMaker k-Nearest-Neighbors (kNN) algorithm on the single time series consisting of the full year of data with a predictor_type of regressor.

B.
Use Amazon SageMaker Random Cut Forest (RCF) on the single time series consisting of the full year of data.

C.
Use the Amazon SageMaker Linear Learner algorithm on the single time series consisting of the full year of data with a predictor_type of regressor.

D.
Use the Amazon SageMaker Linear Learner algorithm on the single time series consisting of the full year of data with a predictor_type of classifier.
","Correct Answer
C. Use the Amazon SageMaker Linear Learner algorithm on the single time series consisting of the full year of data with a predictor_type of regressor.
Explanation
The Amazon SageMaker Linear Learner algorithm is most likely to provide the best results in this scenario. The task is to forecast air quality based on historical data, which is a regression problem. The Linear Learner algorithm is designed for regression tasks and can effectively learn patterns and make predictions based on the given time series data. Using a regressor predictor_type will allow the algorithm to accurately forecast the air quality in parts per million of contaminants for the next 2 days. The other options, such as k-Nearest-Neighbors and Random Cut Forest, may not be as suitable for this specific task.",
90,"A data scientist is working on optimizing a model during the training process by varying multiple parameters. The data scientist observes that, during multiple runs with identical parameters, the loss function converges to different, yet stable, values. What should the data scientist do to improve the training process?

A.
Increase the learning rate. Keep the batch size the same.

B.
Reduce the batch size. Decrease the learning rate

C.
Keep the batch size the same. Decrease the learning rate

D.
Do not change the learning rate. Increase the batch size.
","Correct Answer
B. Reduce the batch size. Decrease the learning rate
Explanation
It is most likely that the loss function is very curvy and has multiple local minima where the training is
getting stuck. Decreasing the batch size would help the data scientist stochastically get out of the local minima
saddles. Decreasing the learning rate would prevent overshooting the global loss function minimum",
91,"A Machine Learning Specialist is using an Amazon SageMaker notebook instance in a private subnet of a corporate VPC. The ML Specialist has important data stored on the Amazon SageMaker notebook instance's Amazon EBS volume, and needs to take a snapshot of that EBS volume. However the ML Specialist cannot find the Amazon
SageMaker notebook instance's EBS volume or Amazon EC2 instance within the VPC. Why is the ML Specialist not seeing the instance visible in the VPC?

A.
Amazon SageMaker notebook instances are based on the EC2 instances within the customer account but they run outside of VPCs.

B.
Amazon SageMaker notebook instances are based on the Amazon ECS service within customer accounts.

C.
Amazon SageMaker notebook instances are based on EC2 instances running within AWS service accounts.

D.
Amazon SageMaker notebook instances are based on AWS ECS instances running within AWS service accounts.
","Correct Answer
C. Amazon SageMaker notebook instances are based on EC2 instances running within AWS service accounts.",
92,"A data engineer needs to create a cost-effective data pipeline solution that ingests unstructured data from various sources and stores it for downstream analytics applications and ML. The solution should include a data store where the processed data is highly available for at least one year so that data analysts and data scientists can run analytics and ML workloads on the most recent data. For compliance reasons, the solution should include both processed and raw data. The raw data does not need to be accessed regularly, but when needed, should be accessible within 24 hours. What solution should the data engineer deploy?

A.
Use Amazon S3 Standard for all raw data. Use Amazon S3 Glacier Deep Archive for all processed data.

B.
Use Amazon S3 Standard for the processed data that is within one year of processing. After one year, use Amazon S3 Glazier for the processed data. Use Amazon S3 Glacier Deep Archive for all raw data.

C.
Use Amazon Elastic File System (Amazon EFS) for processed data is within one year of processing. After one year, use Amazon S3 Standard for the processed data. Use Amazon S3 Glacier Deep Archive for all raw data.

D.
Use Amazon S3 Standard for both the raw and processed data. after one year, use Amazon S3 Glacier Deep Archive for the raw data.
","Correct Answer
B. Use Amazon S3 Standard for the processed data that is within one year of processing. After one year, use Amazon
S3 Glazier for the processed data. Use Amazon S3 Glacier Deep Archive for all raw data.
Explanation
The data engineer should deploy Amazon S3 Standard for the processed data that is within one year of processing.
After one year, they should use Amazon S3 Glacier for the processed data. Additionally, they should use Amazon S3
Glacier Deep Archive for all raw data. This solution ensures that the processed data is highly available for at least one year, allowing data analysts and data scientists to run analytics and ML workloads on the most recent data. The use of Amazon S3 Glacier Deep Archive for raw data ensures compliance and accessibility within 24 hours when needed.",
93,"A Data Science team is designing a dataset repository where it will store a large amount of training data commonly used in its machine learning models. As Data Scientists may create an arbitrary number of new datasets every day the solution has to scale automatically and be cost-effective. Also, it must be possible to explore the data using SQL.
Which storage scheme is MOST adapted to this scenario?

A.
Store datasets as files in Amazon S3.

B.
Store datasets as files in an Amazon EBS volume attached to an Amazon EC2 instance.

C.
Store datasets as tables in a multi-node Amazon Redshift cluster.

D.
Store datasets as global tables in Amazon DynamoDB.
","Correct Answer
A. .Store datasets as files in Amazon S3.
Explanation
Storing datasets as files in Amazon S3 is the most adapted storage scheme for this scenario because it allows for
scalability and cost-effectiveness. With S3, the Data Science team can easily store and retrieve large amounts of
training data without worrying about capacity limitations. Additionally, S3 supports SQL-based querying using
services like Amazon Athena, allowing for easy exploration of the data using SQL. This solution also aligns with the
requirement of being able to create an arbitrary number of new datasets every day, as S3 can handle the storage of
a large number of files.",
94,"A manufacturing company has structured and unstructured data stored in an Amazon S3 bucket. A Machine Learning Specialist wants to use SQL to run queries on this data. Which solution requires the LEAST effort to be able to query this data?

A.
Use AWS Data Pipeline to transform the data and Amazon RDS to run queries.

B.
Use AWS Glue to catalogue the data and Amazon Athena to run queries.

C.
Use AWS Batch to run ETL on the data and Amazon Aurora to run the queries.

D.
Use AWS Lambda to transform the data and Amazon Kinesis Data Analytics to run queries.
","Correct Answer
B. Use AWS Glue to catalogue the data and Amazon Athena to run queries.
Explanation
The correct answer is to use AWS Glue to catalogue the data and Amazon Athena to run queries. AWS Glue is a fully
managed extract, transform, and load (ETL) service that makes it easy to prepare and load data for analytics. It can
automatically discover and catalog data stored in Amazon S3, making it easier to query the data using SQL. Amazon
Athena is an interactive query service that allows you to analyze data directly in Amazon S3 using standard SQL. This
combination of AWS Glue and Amazon Athena requires the least effort as it eliminates the need for manual data
transformation and provides a simple and efficient way to query the data.",
95,"A Machine Learning Specialist is working with a media company to perform classification on popular articles from the company's website. The company is using random forests to classify how popular an article will be before it is published A sample of the data being used is below.

Article Title                   Day of week  URL       Page View
Building a big data..       Tuesday      http:….    1300456
Getting started with..    Tuesday      http:….    1230661
MXNet ML Guide             Thursday     http:….     937291
Intro to NoSQL ..              Monday       http:….     407812

Given the dataset, the Specialist wants to convert the Day-Of_Week column to binary values.
What technique should be used to convert this column to binary values? - ProProfs
A Machine Learning Specialist is working with a media company to perform classification on popular articles from the company's website. The company is using random forests to classify how popular an article will be before it is published A sample of the data being used is below. Given the dataset, the Specialist wants to convert the Day-Of_Week column to binary values. What technique should be used to convert this column to binary values?

A. Binarization

B. One-hot encoding

C. Tokenization

D. Normalization transformation","Correct Answer
B. One-hot encoding
Explanation
The technique that should be used to convert the Day-Of_Week column to binary values is one-hot encoding. Onehot
encoding is a technique used to represent categorical variables as binary vectors. Each category is converted into
a binary column, where a value of 1 represents the presence of that category and a value of 0 represents the
absence. This is commonly used in machine learning algorithms to handle categorical data and allow them to be
used in mathematical calculations.",
96,"A navigation and transportation company is using satellite images to model weather around the world in order to create optimal routes for its ships and planes. The company is using Amazon SageMaker training jobs to build and train its models. However, during training, it takes too long to download the company's 100 GB data from Amazon S3 to the training instance before the training starts. What should the company do to speed up its training jobs while keeping the costs low?

A. Increase the instance size for training

B. Increase the batch size in the model

C. Change the input mode to Pipe

D. Create an Amazon EBS volume with the data on it and attach it to the training job

","Correct Answer
C. Change the input mode to Pipe
Explanation
Changing the input mode to Pipe would speed up the training jobs while keeping the costs low. By using Pipe mode,
the company can stream the data directly from Amazon S3 to the training instance without the need to download
the entire 100 GB data before training starts. This eliminates the time-consuming download process and allows for
faster training. Additionally, it helps in reducing storage costs as there is no need to store the data on the training
instance.",
97,"A Machine Learning Engineer is creating and preparing data for a linear regression model. However, while preparing the data, the Engineer notices that about 20% of the numerical data contains missing values in the same two columns. The shape of the data is 500 rows by 4 columns, including the target column. How could the Engineer
handle the missing values in the data? (Select TWO.)

A.
Remove the rows containing the missing values

B.
Remove the columns containing the missing values

C.
Fill the missing values with zeros

D.
Impute the missing values using regression

E.
Add regularization to the model
","Correct Answers:
C. Fill the missing values with zeros
D. Impute the missing values using regression
Explanation
The Engineer can handle the missing values in two ways. Firstly, they can fill the missing values with zeros, which
means replacing the missing values with the value of zero. Secondly, they can impute the missing values using
regression, which involves using the other available data to predict and fill in the missing values based on a
regression model. These two approaches help to ensure that the missing values are accounted for and do not
negatively impact the linear regression model's performance.",
98,"A Data Scientist at a retail company is using Amazon SageMaker to classify social media posts that mention the company into one of two categories: Posts that require a response from the company, and posts that do not. The Data Scientist is using a training dataset of 10,000 posts, which contains the timestamp, author, and full text of each post. However, the Data Scientist is missing the target labels that are required for training. Which approach can the Data Scientist take to create valid target label data? (Select TWO.)

A.
Ask the social media handling team to review each post using Amazon SageMaker GroundTruth and provide the label

B.
Use the sentiment analysis natural language processing library to determine whether a post requires a response

C.
Use Amazon Mechanical Turk to publish Human Intelligence Tasks that ask Turk workers to label the posts

D.
Use the a priori probability distribution of the two classes. Then, use Monte-Carlo simulation to generate the labels

E.
Use K-Means to cluster posts into various groups, and pick the most frequent word in each group as its label
","Correct Answers: : A,C
A. Ask the social media handling team to review each post using Amazon SageMaker GroundTruth and provide the
label
C. Use Amazon Mechanical Turk to publish Human Intelligence Tasks that ask Turk workers to label the posts
Explanation
The Data Scientist can ask the social media handling team to review each post using Amazon SageMaker
GroundTruth and provide the label. This approach involves manual review and labeling of each post by the team,
ensuring accurate target labels for training. Additionally, the Data Scientist can use Amazon Mechanical Turk to
publish Human Intelligence Tasks that ask Turk workers to label the posts. This crowdsourcing approach allows for a
larger pool of workers to label the posts, increasing efficiency and scalability in generating valid target label data.",
99,"A company is running an Amazon SageMaker training job that will access data stored in its Amazon S3 bucket A compliance policy requires that the data never be transmitted across the internet How should the company set up the job?

A.
Launch the notebook instances in a public subnet and access the data through the public S3 endpoint

B.
Launch the notebook instances in a private subnet and access the data through a NAT gateway

C.
Launch the notebook instances in a public subnet and access the data through a NAT gateway

D.
Launch the notebook instances in a private subnet and access the data through an S3 VPC endpoint.
","Correct Answer
D. Launch the notebook instances in a private subnet and access the data through an S3 VPC endpoint.
Explanation
The company should launch the notebook instances in a private subnet and access the data through an S3 VPC endpoint. This setup ensures that the data is not transmitted across the internet, as required by the compliance policy. By using a private subnet, the instances are not accessible from the public internet. The S3 VPC endpoint allows the instances to securely access the S3 bucket within the VPC, without the need for internet connectivity. This ensures that the data remains within the company's network and complies with the compliance policy.",
100,"A company is interested in building a fraud detection model. Currently, the data scientist does not have a sufficient amount of information due to the low number of fraud cases. Which method is MOST likely to detect the GREATEST number of valid fraud cases?

A.
Oversampling using bootstrapping

B.
Undersampling

C.
Oversampling using SMOTE

D.
Class weight adjustment
","Correct Answer
C. Oversampling using SMOTE
Explanation
With datasets that are not fully populated, the Synthetic Minority Over-sampling Technique (SMOTE) adds new information by adding synthetic data points to the minority class. This technique would be the most effective in this scenario.",
101,"Your marketing department wishes to understand how their products are being represented in the various social media services in which they have active content streams. They would like insights into the reception of a current product line so they can plan for the roll out of a new product in the line in the new future. You have been tasked with creating a service that organizes the social media content by sentiment across all languages so that your marketing department can determine how best to introduce the new product. How would you quickly and most efficiently design and build a service for your marketing team that gives insight into the social media sentiment?

A.
Use the scikit-learn python library to build a sentiment analysis service to provide insight data to the marketing team's internal application platform. Build a dashboard into the application platform using React or Angular.

B.
Use the DetectSentiment Amazon Comprehend API as a service to provide insight data to the marketing team's internal application platform. Build a dashboard into the application platform using React or Angular.

C.
Use the Amazon Lex API as a service to implement the to provide insight data to the marketing team's internal application platform. Build a dashboard into the application platform using React or Angular.

D.
Use Amazon Translate, Amazon Comprehend, Amazon Kinesis, Amazon Athena, and Amazon QuickSight to build a natural-language-processing (NLP)-powered social media dashboard
","Correct Answer
D. Use Amazon Translate, Amazon Comprehend, Amazon Kinesis, Amazon Athena, and Amazon QuickSight to build a
natural-language-processing (NLP)-powered social media dashboard
Explanation 
The best way to quickly and efficiently design and build a service that gives insight into social media sentiment for the marketing team is to use Amazon Translate, Amazon Comprehend, Amazon Kinesis, Amazon Athena, and Amazon QuickSight. These services provide a comprehensive solution for natural language processing (NLP) and data analysis. Amazon Translate can be used to translate social media content into different languages, Amazon Comprehend can be used for sentiment analysis, Amazon Kinesis can be used for real-time data streaming, Amazon Athena can be used for querying and analyzing the data, and Amazon QuickSight can be used for visualizing the insights on a dashboard. This combination of services enables the marketing team to understand the sentiment of their products across different languages and make informed decisions for the roll out of a new product.",
102,"A Data Scientist is working on an application that performs sentiment analysis. The validation accuracy is poor and the Data Scientist thinks that the cause may be a rich vocabulary and a low average frequency of words in the dataset. Which tool should be used to improve the validation accuracy?

A.
Amazon Comprehend syntax analysts and entity detection

B.
Amazon SageMaker BlazingText allow mode

C.
Natural Language Toolkit (NLTK) stemming and stop word removal

D.
Scikit-learn term frequency-inverse document frequency (TF-IDF) vectorizers
","Correct Answer
D. Scikit-learn term frequency-inverse document frequency (TF-IDF) vectorizers
Explanation
The Data Scientist believes that the poor validation accuracy may be due to a rich vocabulary and low average frequency of words in the dataset. In order to improve the accuracy, they should use Scikit-learn term frequencyinverse document frequency (TF-IDF) vectorizers. TF-IDF is a technique that assigns weights to words based on their frequency in a document and their rarity in the entire dataset. By using TF-IDF vectorizers, the Data Scientist can give more importance to the words that are both frequent in a document and rare in the dataset, which can help improve the accuracy of the sentiment analysis application.",
103,"A Machine Learning Specialist is preparing data for training on Amazon SageMaker. The Specialist is using one of the SageMaker built-in algorithms for the training. The dataset is stored in .CSV format and is transformed into a numpy.array, which appears to be negatively affecting the speed of the training. What should the Specialist do to
optimize the data for training on SageMaker?

A.
Use the SageMaker batch transform feature to transform the training data into a DataFrame

B.
Use AWS Glue to compress the data into the Apache Parquet format

C.
Transform the dataset into the Recordio protobuf format

D.
Use the SageMaker hyperparameter optimization feature to automatically optimize the data

","Correct Answer
C. Transform the dataset into the Recordio protobuf format
Explanation
The Specialist should transform the dataset into the Recordio protobuf format. This format is optimized for highperformance, efficient data storage and retrieval, which can improve the speed of training on SageMaker.",
104,"A social networking organization wants to analyze all the comments and likes from its users to flag offensive language on the site. The organization's data science team wants to use a Long Short-term Memory (LSTM) architecture to classify the raw sentences from the comments into one of two categories: offensive and
nonoffensive. What should the team do to prepare the data for the LSTM?

A.
Convert the individual sentences into sequences of words. Use those as the input.

B.
Convert the individual sentences into numerical sequences starting from the number 1  for each word in a sentence. Use the sentences as the input.

C.
Vectorize the sentences. Transform them into numerical sequences. Use the sentences as the input.

D.
Vectorize the sentences. Transform them into numerical sequences with a padding. Use the sentences as the input.
","Correct Answer
D. Vectorize the sentences. Transform them into numerical sequences with a padding. Use the sentences as the input.
Explanation
To prepare the data for the LSTM, the team should vectorize the sentences by transforming them into numerical sequences. Additionally, padding should be applied to ensure that all sequences have the same length. This is important because LSTMs require fixed-length input. By vectorizing and padding the sentences, the data can be effectively processed by the LSTM model for classification.",
105,"Which probability distribution would describe the likelihood of flipping a coin ""heads""?

A.
Bernoulli Distribution

B.
Normal Distribution

C.
Poisson Distribution

D.
Binomial Distribution
","Correct Answer
D. Binomial Distribution
Explanation
The likelihood of flipping a coin ""heads"" can be described by the Binomial Distribution. This distribution is used when there are two possible outcomes (in this case, heads or tails) and each flip is independent. The Binomial Distribution calculates the probability of a certain number of successes (in this case, heads) in a fixed number of trials (the number of times the coin is flipped).
",
106,"You work for a real estate company where you are building a machine learning model to predict the prices of houses.
You are using a regression decision tree. As you train your model you see that it is overfitted to your training data and that it doesn't generalize well to unseen data. How can you improve your situation and get better training results in the most efficient way?

A.
Use a random forest by building multiple randomized decision trees and averaging their outputs to get the predictions of the housing prices.

B.
Gather additional training data that gives a more diverse representation of the housing price data.

C.
Use the “dropout” technique to penalize large weights and prevent overfitting.

D.
Use feature selection to eliminate irrelevant features and iteratively train your model until you eliminate the overfitting.

","Correct Answer
A. Use a random forest by building multiple randomized decision trees and averaging their outputs to get the
predictions of the housing prices.
Explanation
Using a random forest by building multiple randomized decision trees and averaging their outputs can improve the situation and provide better training results. Random forests help to reduce overfitting by introducing randomness into the model. By building multiple decision trees with different subsets of the data and features, the model can learn from different perspectives and make more accurate predictions. Averaging the outputs of these trees helps to reduce the impact of individual overfitted trees and provides a more generalized prediction for unseen data.",
107,"A Machine Learning Specialist is configuring Amazon SageMaker so multiple Data Scientists can access notebooks, train models, and deploy endpoints. To ensure the best operational performance, the Specialist needs to be able to track how often the Scientists are deploying models, GPU and CPU utilization on the deployed SageMaker endpoints, and all errors that are generated when an endpoint is invoked. Which services are integrated with Amazon SageMaker to track this information? (Select TWO.)

A.
AWS CloudTrail

B.
AWS Health

C.
AWS Trusted Advisor

D.
Amazon CloudWatch

E.
AWS Config
","Correct Answers:
A. AWS CloudTrail
D. Amazon CloudWatch
Explanation
The correct answer is AWS CloudTrail and Amazon CloudWatch. AWS CloudTrail is used to track API activity and monitor actions taken by users, including model deployments and endpoint invocations. Amazon CloudWatch is used to monitor resource utilization, such as GPU and CPU utilization on the deployed SageMaker endpoints. AWS Health, AWS Trusted Advisor, and AWS Config are not directly integrated with Amazon SageMaker for tracking this information.",
108,"A Data Scientist needs to migrate an existing on-premises ETL process to the cloud. The current process runs at regular time intervals and uses PySpark to combine and format multiple large data sources into a single consolidated output for downstream processing. The Data Scientist has been given the following requirements to the cloud
solution:
Combine multiple data sources.
Reuse existing PySpark logic.
Run the solution on the existing schedule.
Minimize the number of servers that will need to be managed.
Which architecture should the Data Scientist use to build this solution?

A.
Write the raw data to Amazon S3. Schedule an AWS Lambda function to submit a Spark step to a persistent Amazon EMR cluster based on the existing schedule. 
Use the existing PySpark logic to run the ETL job on the EMR cluster. 
Output the results to a “processed” location in Amazon S3 that is accessible for downstream use.

B.
Write the raw data to Amazon S3. Create an AWS Glue ETL job to perform the ETL processing against the input data. 
Write the ETL job in PySpark to leverage the existing logic. Create a new AWS Glue trigger to trigger the ETL job based on the existing schedule. Configure the output target of the ETL job to write to a “processed” location in Amazon S3 that is accessible for downstream use.

C.
Write the raw data to Amazon S3. Schedule an AWS Lambda function to run on the existing schedule and process the input data from Amazon S3. Write the Lambda logic in Python and implement the existing PySpark logic to perform the ETL process. Have the Lambda function output the results to a “processed” location in Amazon S3 that is
accessible for downstream use.

D.
Use Amazon Kinesis Data Analytics to stream the input data and perform real-time SQL queries against the stream to carry out the required transformations within the stream. Deliver the output results to a “processed” location in Amazon S3 that is accessible for downstream use.
","Correct Answer : B

B. Write the raw data to Amazon S3. Create an AWS Glue ETL job to perform the ETL processing against the input data. Write the ETL job in PySpark to leverage the existing logic. Create a new AWS Glue trigger to trigger the ETL job based on the existing schedule. Configure the output target of the ETL job to write to a “processed” location in Amazon S3 that is accessible for downstream use.
Explanation
The Data Scientist should use the architecture described in option 2. This option suggests writing the raw data to Amazon S3 and using AWS Glue ETL job to perform the ETL processing. By writing the ETL job in PySpark, the existing logic can be leveraged. A new AWS Glue trigger can be created to trigger the ETL job based on the existing schedule.
The output target of the ETL job can be configured to write to a ""processed"" location in Amazon S3, which is accessible for downstream use. This architecture meets all the given requirements, including combining multiple data sources, reusing existing PySpark logic, running on the existing schedule, and minimizing the number of managed servers.",
109,"While working on a neural network project, a Machine Learning Specialist discovers that some features in the data have very high magnitude resulting in this data being weighted more in the cost function. What should the Specialist do to ensure better convergence during backpropagation?

A.
Dimensionality reduction

B.
Data normalization

C.
Model regularization

D.
Data augmentation for the minority class
","Correct Answer
B. Data normalization
Explanation
Data normalization is the process of scaling the data to a standard range. In this case, the high magnitude of some features can cause the neural network to give more importance to those features, leading to slower convergence during backpropagation. By normalizing the data, the features will be on a similar scale, allowing the neural network to learn more effectively and converge faster. This helps to prevent any one feature from dominating the cost function and ensures better convergence during backpropagation.",
110,"A machine learning engineer is preparing a data frame for a supervised learning task with the Amazon SageMaker Linear Learner algorithm. The ML engineer notices the target label classes are highly imbalanced and multiple feature columns contain missing values. The proportion of missing values across the entire data frame is less than 5%. What should the ML engineer do to minimize bias due to missing values?

A.
Replace each missing value by the mean or median across non-missing values in same row.

B.
Delete observations that contain missing values because these represent less than 5% of the data

C.
Replace each missing value by the mean or median across non-missing values in the same column.

D.
For each feature, approximate the missing values using supervised learning based on other features.
","Correct Answer
D. For each feature, approximate the missing values using supervised learning based on other features.
Explanation
Use supervised learning to predict missing values based on the values of other features. Different supervised learning approaches might have different performances, but any properly implemented supervised learning approach should provide the same or better approximation than mean or median approximation, as proposed in responses A and C. Supervised learning applied to the imputation of missing values is an active field of research.",
111,"You are a data scientist working for a cancer screening center. The center has gathered data on many patients that have been screened over the years. The data is obviously skewed toward true negative results, as most screened patients don't have cancer. You are evaluating several machine learning models to decide which model best predicts true positives when using your cancer screening data. You have split your data into a 70/30 ratio of training set to test set. You now need to decide which metric to use to evaluate your models. Which metric will most accurately determine the model best suited to solve your classification problem?

A.
ROC Curve

B.
Precision

C.
Recall

D.
PR Curve
","Correct Answer
D. PR Curve
Explanation
The PR Curve is the most suitable metric to determine the model best suited for the classification problem in this scenario. Since the data is skewed towards true negative results, precision and recall are more appropriate metrics than the ROC curve. Precision measures the proportion of true positive predictions out of all positive predictions, while recall measures the proportion of true positive predictions out of all actual positive cases. The PR Curve combines both precision and recall, providing a more accurate evaluation of the model's performance in identifying true positives.",
112,"A Data Scientist wants to tune the hyperparameters of a machine learning model to improve the model's F1score. 
What technique can be used to achieve this desired outcome on Amazon SageMaker? (Select TWO)

A.
Grid Search

B.
Random Search

C.
Breadth First Search

D.
Bayesian optimization

E.
Depth first search
","Correct Answers: : B,D
B. Random Search
D. Bayesian optimization
Explanation
Random Search and Bayesian optimization are two techniques that can be used to tune the hyperparameters of a machine learning model on Amazon SageMaker to improve the model's F1 score. Random Search involves randomly selecting combinations of hyperparameters from a predefined search space and evaluating their performance. Bayesian optimization, on the other hand, uses a probabilistic model to find the optimal set of hyperparameters by iteratively exploring the search space based on previous evaluations. Both techniques can help identify the best hyperparameter values that maximize the F1 score.",
113,"A Machine Learning Specialist working for an online fashion company wants to build a data ingestion solution for the company's Amazon S3-based data lake. The Specialist wants to create a set of ingestion mechanisms that will enable future capabilities comprised of:
Real-time analytics
Interactive analytics of historical data
Clickstream analytics
Product recommendations
Which services should the Specialist use?

A.
AWS Glue as the data dialog; Amazon Kinesis Data Streams and Amazon Kinesis Data Analytics for real-time data insights; Amazon Kinesis Data Firehose for delivery to Amazon ES for clickstream analytics; Amazon EMR to generate personalized product recommendations.

B.
Amazon Athena as the data catalog; Amazon Kinesis Data Streams and Amazon Kinesis Data Analytics for near-real time data insights; Amazon Kinesis Data Firehose for clickstream analytics; AWS Glue to generate personalized product recommendations.

C.
AWS Glue as the data catalog; Amazon Kinesis Data Streams and Amazon Kinesis Data Analytics for historical data insights; Amazon Kinesis Data Firehose for delivery to Amazon ES for clickstream analytics; Amazon EMR to generate personalized product recommendations. 

D.
Amazon Athena as the data catalog; Amazon Kinesis Data Streams and Amazon Kinesis Data Analytics for historical data insights; Amazon DynamoDB streams for clickstream analytics; AWS Glue to generate personalized product recommendations.
","Correct Answer
A. AWS Glue as the data dialog; Amazon Kinesis Data Streams and Amazon Kinesis Data Analytics for real-time data insights; Amazon Kinesis Data Firehose for delivery to Amazon ES for  clickstream analytics; Amazon EMR to generate personalized product recommendations.
Explanation
The Specialist should use AWS Glue as the data catalog to manage the metadata of the data lake. They should use Amazon Kinesis Data Streams and Amazon Kinesis Data Analytics for real-time data insights, allowing them to process and analyze streaming data in real-time. They should also use Amazon Kinesis Data Firehose to deliver the clickstream data to Amazon ES for clickstream analytics. Lastly, they should use Amazon EMR to generate personalized product recommendations by processing and analyzing the data in the data lake.
",
114,"A company has collected customer comments on its products, rating them as safe or unsafe, using decision trees.  The training dataset has the following features: id, date, full review, full review summary, and a binary safe/unsafe tag. During training, any data sample with missing features was dropped. In a few instances, the test set was found to be missing the full review text field. For this use case, which is the most effective course of action to address test data samples with missing features?

A.
Drop the test samples with missing full review text fields, and then run through the test set.

B.
Copy the summary text fields and use them to fill in the missing full review text fields, and then run through the test set.

C.
Use an algorithm that handles missing data better than decision trees.

D.
Generate synthetic data to fill in the fields that are missing data, and then run through the test set.
","Correct Answer
B. Copy the summary text fields and use them to fill in the missing full review text fields, and then run through the test set.
Explanation
In this case, a full review summary usually contains the most descriptive phrases of the entire review and is a valid stand-in for the missing full review text field.",
115,"An agency collects census information within a country to determine healthcare and social program needs by province and city. The census form collects responses for approximately 500 questions from each citizen. Which combination of algorithms would provide the appropriate insights? (Select TWO)

A.
The factorization machines (FM) algorithm

B.
The Latent Dirichlet Allocation (LDA) algorithm

C.
The principal component analysis (PCA) algorithm

D.
The k-means algorithm

E.
The Random Cut Forest (RCF) algorithm
","Correct Answers:
C. The principal component analysis (PCA) algorithm
D. The k-means algorithm
Explanation
The principal component analysis (PCA) algorithm is suitable for this task as it can reduce the dimensionality of the data and identify the most important variables that contribute to the variance in the dataset. This can help in identifying patterns and relationships within the census information. The k-means algorithm can be used to cluster the data based on similarities, which can be useful in grouping provinces and cities with similar healthcare and social program needs. These algorithms together can provide valuable insights for determining healthcare and social program needs by province and city based on the census information.",
116,"A Machine Learning Specialist has completed a proof of concept for a company using a small data sample and now the Specialist is ready to implement an end-to-end solution in AWS using Amazon SageMaker. The historical training data is stored in Amazon RDS. Which approach should the Specialist use for training a model using that data?

A.
Write a direct connection to the SQL database within the notebook and pull data in.

B.
Push the data from Microsoft SQL Server to Amazon S3 using an AWS Data Pipeline and provide the S3 location within the notebook.

C.
Move the data to Amazon DynamoDB and set up a connection to DynamoDB within the notebook to pull data in.

D.
Move the data to Amazon ElastiCache using AWS DMS and set up a connection within the notebook to pull data in for fast access.
","Correct Answer
B. Push the data from Microsoft SQL Server to Amazon S3 using an AWS Data Pipeline and provide the S3 location within the notebook.
Explanation
The Specialist should push the data from Microsoft SQL Server to Amazon S3 using an AWS Data Pipeline and provide the S3 location within the notebook. This approach allows for efficient and scalable storage of the historical training data in Amazon S3, which can then be easily accessed and used for training the model in Amazon SageMaker. It also ensures that the data is securely stored and can be easily shared and accessed by other services or users within the AWS environment.",
117,"A Data Scientist wants to use the Amazon SageMaker hyperparameter tuning job to automatically tune a random forest model. What API does the Amazon SageMaker SDK use to create and interact with the Amazon SageMaker hyperparameter tuning jobs?

A.
YperparameterTunerJob()

B.
HyperparameterTuner()

C.

HyperparameterTuningJobs()

D.
Hyperparameter()
","Correct Answer : B
B. HyperparameterTuner()
Explanation
The Amazon SageMaker SDK uses the HyperparameterTuner() API to create and interact with the Amazon SageMaker hyperparameter tuning jobs. This API allows the data scientist to automate the tuning process for their random forest model, optimizing the hyperparameters to improve the model's performance.",
118,"In AWS SageMaker, what feature allows you to distribute machine learning model training across multiple instances and is designed for large-scale distributed training?

A. SageMaker Data Wrangler

B. SageMaker Model Monitor

C. SageMaker Multi-Model Endpoints

D. SageMaker Distributed Training
","Correct Answer
D. SageMaker Distributed Training
Explanation
SageMaker Distributed Training is a feature within Amazon SageMaker that enables large-scale distributed training of machine learning models across multiple instances. This advanced capability is particularly useful for handling large datasets and complex model training scenarios, making it an essential tool for scaling machine learning workflows in AWS.",
119,"A Data Scientist created a correlation matrix between nine variables and the target variable. The correlation coefficient between two of the numerical variables, variable 1 and variable 5, is -0.95. How should the Data Scientist interpret the correlation coefficient?

A. As variable 1 increases, variable 5 increases

B. As variable 1 increases, variable 5 decreases

C. Variable 1 does not have any influence on variable 5

D. The data is not sufficient to make a well-informed interpretation
","Correct Answer
B. As variable 1 increases, variable 5 decreases
Explanation
The correlation coefficient of -0.95 indicates a strong negative correlation between variable 1 and variable 5. This means that as variable 1 increases, variable 5 tends to decrease. The closer the correlation coefficient is to -1, the stronger the negative correlation. Therefore, the Data Scientist can interpret that there is a strong inverse relationship between variable 1 and variable 5.",
120,"A manufacturing company has a large set of labeled historical sales data The manufacturer would like to predict how many units of a particular part should be produced each quarter. Which machine learning approach should be used to solve this problem?

A.
Logistic regression

B.
Random Cut Forest (RCF)

C.
Principal component analysis (PCA)

D.
Linear regression
","Correct Answer
D. Linear regression
Explanation
Linear regression is the appropriate machine learning approach to solve the problem of predicting the number of units of a particular part that should be produced each quarter. Linear regression is used for predicting a continuous
numerical value, which aligns with the problem of predicting the quantity of units to be produced. Logistic regression, Random Cut Forest (RCF), and Principal Component Analysis (PCA) are not suitable in this case because they are used for different types of problems such as classification, anomaly detection, and dimensionality reduction, respectively.",
121,"An insurance company needs to automate claim compliance reviews because human reviews are expensive and error-prone. The company has a large set of claims and a compliance label for each. Each claim consists of a few sentences in English, many of which contain complex related information. Management would like to use Amazon SageMaker built-in algorithms to design a machine learning supervised model that can be trained to read each claim and predict if the claim is compliant or not. Which approach should be used to extract features from the claims to be used as inputs for the downstream supervised task?

A.
Derive a dictionary of tokens from claims in the entire dataset. Apply one-hot encoding to tokens found in each claim of the training set. Send the derived features space as inputs to an Amazon SageMaker built in supervised learning algorithm.

B.
Apply Amazon SageMaker BlazingText in Word2Vec mode to claims in the training set. Send the derived features space as inputs for the downstream supervised task.

C.
Apply Amazon SageMaker BlazingText in classification mode to labeled claims in the training set to derive features for the claims that correspond to the compliant and non-compliant labels, respectively.

D.
Apply Amazon SageMaker Object2Vec to claims in the training set. Send the derived features space as inputs for the downstream supervised task.
","Correct Answer
D. Apply Amazon SageMaker Object2Vec to claims in the training set. Send the derived features space as inputs for the downstream supervised task.
Explanation
Amazon SageMaker Object2Vec generalizes the Word2Vec embedding technique for words to more complex objects, such as sentences and paragraphs. Since the supervised learning task is at the level of whole claims, for which there are labels, and no labels are available at the word level, Object2Vec needs be used instead of Word2Vec.",
122,"An analytics company wants to use a fully managed service that automatically scales to handle the transfer of its Apache web logs, syslogs, text and videos on their webserver to Amazon S3 with minimum transformation. What service can be used for this process?

A.
Kinesis Data Streams

B.
Kinesis Firehose

C.
Kinesis Data Analytics

D.
Amazon Kinesis Video Streams
","Correct Answer
B. Kinesis Firehose
Explanation
Kinesis Firehose is the correct answer for this question. Kinesis Firehose is a fully managed service that automatically scales to handle the transfer of data, such as Apache web logs, syslogs, text, and videos, from various sources to Amazon S3. It requires minimum transformation, making it suitable for the given scenario where the analytics company wants to transfer their web logs, syslogs, text, and videos to Amazon S3 without extensive data manipulation.",
123,"A video streaming company is looking to create a personalized experience for its customers on its platform. The company wants to provide recommended videos to stream based on what other similar users watched previously. 
To this end, it is collecting its platform's clickstream data using an ETL pipeline and storing the logs and syslogs in Amazon S3. What kind of algorithm should the company use to create the simplest solution in this situation?

A.
Regression

B.
Classification

C.
Recommender system

D.
Reinforcement learning
","Correct Answer
C. Recommender system
Explanation
The company should use a recommender system algorithm to create a personalized experience for its customers. A recommender system analyzes clickstream data and user behavior to provide recommendations based on what other similar users watched previously. This algorithm would be the simplest solution for the company to implement in order to provide recommended videos to stream on its platform.",
124,"A real estate company wants to provide its customers with a more accurate prediction of the final sale price for houses they are considering in various cities. To do this, the company wants to use a fully connected neural network trained on data from the previous ten years of home sales, as well as other features. What kind of machine learning problem does this situation represent?

A.
Regression

B.
Classification

C.
Recommender system

D.
Reinforcement learning
","Correct Answer
A. Regression
Explanation
This situation represents a regression problem. Regression is a type of machine learning problem where the goal is to predict a continuous numerical value. In this case, the real estate company wants to predict the final sale price of houses, which is a continuous variable. By using a fully connected neural network trained on previous home sales data, the company can make more accurate predictions for their customers.",
125,"A Machine Learning Specialist is developing a custom video recommendation model for an application The dataset used to train this model is very large with millions of data points and is hosted in an Amazon S3 bucket The Specialist wants to avoid loading all of this data onto an Amazon SageMaker notebook instance because it would take hours to move and will exceed the attached 5 GB Amazon EBS volume on the notebook instance.
Which approach allows the Specialist to use all the data to train the model?

A. Load a smaller subset of the data into the SageMaker notebook and train locally

B. Confirm that the training code is executing and the model parameters seem reasonable

C. Initiate a SageMaker training job using the full dataset from the S3 bucket using Pipe input mode.

D. Launch an Amazon EC2 instance with an AWS Deep Learning AMI and attach the S3 bucket to the instance

E. Train on a small amount of the data to verify the training code and hyperparameter

F. Go back to Amazon SageMaker and train using the full dataset

G. Use AWS Glue to train a model using a small subset of the data to confirm that the data will be compatiblewith Amazon SageMaker

H. Initiate a SageMaker training job using the full dataset from the S3 bucket usingPipe input mode.

I. Load a smaller subset of the data into the SageMaker notebook and train locally

J. Confirm that the training code is executing and the model parameters seem reasonable

K. Launch an Amazon EC2 instance with an AWS Deep Learning AMI and attach the S3 bucket to train the full dataset.
","Correct Answer
A. Load a smaller subset of the data into the SageMaker notebook and train locally. Confirm that the training code is executing and the model parameters seem reasonable. Initiate a SageMaker training job using the full dataset from the S3 bucket using Pipe input mode.
Explanation
To avoid loading the entire large dataset onto the limited storage of the SageMaker notebook instance, the Machine Learning Specialist should load a smaller subset of the data into the notebook and train locally. This allows them to confirm that the training code is executing correctly and the model parameters are reasonable. Once this is verified, they can initiate a SageMaker training job using the full dataset from the S3 bucket using Pipe input mode. This approach allows the Specialist to use all the data for training without exceeding the storage limitations of the notebook instance.",
126,"A Mobile Network Operator is building an analytics platform to analyze and optimize a company's operations using Amazon Athena and Amazon S3. The source systems send data in CSV format in real time. The Data Engineering team wants to transform the data to the Apache Parquet format before storing it on Amazon S3. Which solution takes the LEAST effort to implement?

A.
Ingest CSV data using Apache Kafka Streams on Amazon EC2 instances and use Kafka Connect S3 to serialize data as Parquet.

B.
Ingest CSV data from Amazon Kinesis Data Streams and use Amazon Glue to convert data into Parquet.

C.
Ingest CSV data using Apache Spark Structured Streaming in an Amazon EMR cluster and use Apache Spark to convert data into Parquet.

D.
Ingest CSV data from Amazon Kinesis Data Streams and use Amazon Kinesis Data Firehose to convert data into Parquet.
","Correct Answer
D. Ingest CSV data from Amazon Kinesis Data Streams and use Amazon Kinesis Data Firehose to convert data into Parquet.
Explanation
The solution that takes the least effort to implement is to ingest CSV data from Amazon Kinesis Data Streams and use Amazon Kinesis Data Firehose to convert the data into Parquet format. This solution eliminates the need for managing and configuring additional services like Apache Kafka Streams, Apache Spark, or Amazon Glue. Amazon Kinesis Data Firehose simplifies the process by automatically converting the CSV data into Parquet format and storing it on Amazon S3.
",
127,"A manufacturing company wants to increase the longevity of its factory machines by predicting when a machine part is about to stop working, jeopardizing the health of the machine. The company's team of Data Scientists will build an ML model to accomplish this goal. The model will be trained on data made up of consumption metrics from similar factory machines, and will span a time frame from one hour before a machine part broke down to five minutes after the part degraded. What kind of machine learning algorithm should the company use to build this model?

A.
Amazon SageMaker DeepAR

B.
SciKit Learn Regression

C.
Convolutional neural network (CNN)

D.
Scikit Learn Random Forest
","Correct Answer
A. Amazon SageMaker DeepAR
Explanation
The company should use Amazon SageMaker DeepAR algorithm to build the model. DeepAR is a time series forecasting algorithm that is specifically designed for predicting future values based on historical data. In this case, the algorithm can be trained on the consumption metrics of similar factory machines to predict when a machine part is about to stop working. The algorithm's ability to handle time series data and capture temporal dependencies makes it suitable for this task.",
128,"A Machine Learning Specialist uploads a dataset to an Amazon S3 bucket protected with server-side encryption using AWS KMS. How should the ML Specialist define the Amazon SageMaker notebook instance so it can read the same dataset from Amazon S3?
A.
Define security group(s) to allow all HTTP inbound/outbound traffic and assign those security group(s) to the Amazon SageMaker notebook instance.

B.
?onfigure the Amazon SageMaker notebook instance to have access to the VPC. Grant permission in the KMS key policy to the notebook's KMS role.

C.
Assign an IAM role to the Amazon SageMaker notebook with S3 read access to the dataset. Grant permission in the KMS key policy to that role.

D.
Assign the same KMS key used to encrypt data in Amazon S3 to the Amazon SageMaker notebook instance.
","Correct Answer
C. Assign an IAM role to the Amazon SageMaker notebook with S3 read access to the dataset. Grant permission in the KMS key policy to that role.
Explanation
https://docs.aws.amazon.com/kms/latest/developerguide/control-access-overview.html#managing-access",
129,"A financial services company is building a robust serverless data lake on Amazon S3. The data lake should be flexible and meet the following requirements:
Support querying old and new data on Amazon S3 through Amazon Athena and Amazon Redshift Spectrum.
Support event-driven ETL pipelines.
Provide a quick and easy way to understand metadata.
Which approach meets these requirements?

A.
Use an AWS Glue crawler to crawl S3 data, an AWS Lambda function to trigger an AWS Glue ETL job, and an AWS Glue Data catalog to search and discover metadata.

B.
Use an AWS Glue crawler to crawl S3 data, an AWS Lambda function to trigger an AWS Batch job, and an external Apache Hive metastore to search and discover metadata.

C.
Use an AWS Glue crawler to crawl S3 data, an Amazon CloudWatch alarm to trigger an AWS Batch job, and an AWS Glue Data Catalog to search and discover metadata.

D.
Use an AWS Glue crawler to crawl S3 data, an Amazon CloudWatch alarm to trigger an AWS Glue ETL job, and an external Apache Hive metastore to search and discover  metadata.
","Correct Answer : A
A. Use an AWS Glue crawler to crawl S3 data, an AWS Lambda function to trigger an AWS Glue ETL job, and an AWS Glue Data catalog to search and discover metadata.
Explanation
This approach meets the requirements because it utilizes AWS Glue, which is a fully managed extract, transform, and load (ETL) service. The AWS Glue crawler is used to automatically discover and catalog metadata about the data in the S3 data lake. An AWS Lambda function is used to trigger the AWS Glue ETL job, which allows for event-driven ETL pipelines. The AWS Glue Data Catalog is used to search and discover metadata, providing a quick and easy way to understand the data lake's metadata. This approach also aligns with the requirement of supporting querying old and new data through Amazon Athena and Amazon Redshift Spectrum.",
130,"A Machine Learning Specialist is using Apache Spark for pre-processing training data As part of the Spark pipeline, the Specialist wants to use Amazon SageMaker for training a model and hosting it. Which of the following would the Specialist do to integrate the Spark application with SageMaker? (Select THREE)

A.
Download the AWS SDK for the Spark environment.

B.
Install the SageMaker Spark library in the Spark environment.

C.
Use the appropriate estimator from the SageMaker Spark Library to train a model.

D.
Compress the training data into a ZIP file and upload it to a pre-defined Amazon S3 bucket.

E.
Use the SageMaker Model transform method to get inferences from the model hosted in SageMaker.

F.
Convert the DataFrame object to a CSV file, and use the CSV file as input for obtaining inferences from SageMaker.
","Correct Answers:
B. Install the SageMaker Spark library in the Spark environment.
C. Use the appropriate estimator from the SageMaker Spark Library to train a model.
E. Use the SageMaker Model transform method to get inferences from the model hosted in SageMaker.
Explanation
To integrate the Spark application with SageMaker, the Machine Learning Specialist would need to perform the following steps: 

1) Install the SageMaker Spark library in the Spark environment, which allows for seamless integration between Spark and SageMaker. 

2) Use the appropriate estimator from the SageMaker Spark Library to train a model, which provides a high-level API for training models on SageMaker using Spark. 

3) Use the SageMaker Model transform method to get inferences from the model hosted in SageMaker, which allows for real-time inference on new data using the trained model.",
131,"A Machine Learning Specialist is working with a large cyber security company that manages security events in real time for companies around the world. The cybersecurity company wants to design a solution that will allow it to use machine learning to score malicious events as anomalies on the data as it is being ingested. The company also wants be able to save the results in its data lake for later processing and analysis. What is the MOST efficient way to accomplish these tasks?

A.
Ingest the data using Amazon Kinesis Data Firehose, and use Amazon Kinesis Data  Analytics Random Cut Forest (RCF) for anomaly detection. Then use Kinesis Data Firehose to stream the results to Amazon S3.

B.
Ingest the data into Apache Spark Streaming using Amazon EMR. and use Spark MLlib with k-means to perform anomaly detection. Then store the results in an Apache Hadoop Distributed File System (HDFS) using Amazon EMR with a replication factor of three as the data lake.

C.
Ingest the data and store it in Amazon S3 Use AWS Batch along with the AWS Deep Learning AMIs to train a k-means model using TensorFlow on the data in Amazon S3.

D.
Ingest the data and store it in Amazon S3. Have an AWS Glue job that is triggered on demand transform the new data. Then use the built-in Random Cut Forest (RCF) model within Amazon SageMaker to detect anomalies in the data.
","Correct Answer
A. Ingest the data using Amazon Kinesis Data Firehose, and use Amazon Kinesis Data Analytics Random Cut Forest (RCF) for anomaly detection. Then use Kinesis Data Firehose to stream the results to Amazon S3.
Explanation
The most efficient way to accomplish the tasks of ingesting and analyzing the data in real-time is by using Amazon Kinesis Data Firehose to ingest the data and Amazon Kinesis Data Analytics Random Cut Forest (RCF) for anomaly detection. After detecting the anomalies, the results can be streamed to Amazon S3 using Kinesis Data Firehose. This approach allows for real-time analysis and storage of the results in a scalable and efficient manner.
",
132,"A security and networking company wants to use ML to flag certain IP addresses that have been known to send spam and phishing information. The company wants to build an ML model based on previous user feedback indicating whether specific IP addresses have been connected to a website designed for spam and phishing. What is the simplest solution that the company can implement?

A.
Regression

B.
Classification

C.
Natural language processing (NLP)

D.
A rule-based solution should be used instead of ML
","Correct Answer
D. A rule-based solution should be used instead of ML
Explanation
A rule-based solution should be used instead of ML because the company already has specific criteria (previous user feedback) to identify IP addresses connected to spam and phishing websites. ML models require training data, which may not be readily available in this case. By using a rule-based solution, the company can set predefined rules based on the feedback to flag the IP addresses without the need for ML algorithms.",
133,"A Data Scientist working for an autonomous vehicle company is building an ML model to detect and label people and various objects (for instance, cars and traffic signs) that may be encountered on a street. The Data Scientist has a dataset made up of labeled images, which will be used to train their machine learning model. What kind of ML algorithm should be used?

A.
Image classification

B.
Instance segmentation

C.
Image localization

D.
Semantic segmentation
","Correct Answer : B
B. Instance segmentation
Explanation
Instance segmentation should be used in this scenario. 
Instance segmentation not only classifies objects in an image but also provides a pixel-level mask for each individual object. This is important in the context of autonomous vehicles as it allows for accurate detection and labeling of people and various objects on the street. 

Image classification would only classify the entire image, while image localization would only provide bounding boxes around objects. 

Semantic segmentation would classify pixels into different categories but would not differentiate between individual objects.",
