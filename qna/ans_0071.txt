Answers : D

Dropout layers helps to prevent overfitting
ReLU is an activation function
